{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, logging\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197d4a",
   "metadata": {},
   "source": [
    "# üì• Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9cf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "archaic_sentences = df[\"Sentence\"].dropna().tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f795e7",
   "metadata": {},
   "source": [
    "# üë®‚Äçüè´ Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26658fb1",
   "metadata": {},
   "source": [
    "## Parametri fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs=10\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#model_name = \"bigscience/bloomz-3b\"\n",
    "\n",
    "nuovi_token_max=100\n",
    "temperatura=0.7\n",
    "max_translations=0 # 0 = no limit, >0 = max number of translations to generate\n",
    "\n",
    "def getPrompt(archaic_sentence):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {archaic_sentence}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf028d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b045c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore nel file fine_tuning/csvs\\canzoniere.csv: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c09eeae93ab4baaabcf679394982744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# üìÅ Percorso alla cartella con i file CSV\n",
    "cartella = \"fine_tuning/csvs\"  \n",
    "\n",
    "# üîç Prende tutti i file .csv nella cartella\n",
    "csv_files = glob.glob(os.path.join(cartella, \"*.csv\"))\n",
    "\n",
    "# üì¶ Carica e concatena tutti i file\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel file {file}: {e}\")\n",
    "\n",
    "# üìö Unione verticale\n",
    "df_finale = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# üíæ Salvataggio\n",
    "df_finale.to_csv(\"dataset_concatenato.csv\", index=False)\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"dataset_concatenato.csv\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94a08",
   "metadata": {},
   "source": [
    "## Modello e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39718bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561af239271d489b8aa47810e22f5f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Per evitare errori su padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6b9",
   "metadata": {},
   "source": [
    "## Configurazione Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4322b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a6df",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca686f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ce11492b9b47a7afde9f6bb0cc7e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251db836f52d43fdb39985d6445dc174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = getPrompt(example['text'])\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"translation\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(format_prompt, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"test\": dataset[\"test\"].map(format_prompt, remove_columns=dataset[\"test\"].column_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb0fe",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510db0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\colam\\AppData\\Local\\Temp\\ipykernel_26204\\958734066.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_debug()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lora-itmoderno\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=training_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,        # ‚úÖ abilita tqdm\n",
    "    report_to=\"none\",          # evita warning da WandB o altri\n",
    "    logging_dir=\"./logs\",      # facoltativo\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "def compute_metrics(eval_preds):\n",
    "    return {} \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9d74",
   "metadata": {},
   "source": [
    "## Avvia Fine Tuning & Salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7560514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 133\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 90\n",
      "  Number of trainable parameters = 3,407,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 10:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.034185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.930822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.732400</td>\n",
       "      <td>1.682987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.732400</td>\n",
       "      <td>1.597075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.608600</td>\n",
       "      <td>1.578524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.608600</td>\n",
       "      <td>1.538317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.336100</td>\n",
       "      <td>1.495402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.336100</td>\n",
       "      <td>1.465212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.147300</td>\n",
       "      <td>1.458198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.147300</td>\n",
       "      <td>1.449009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-9\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-9\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-9\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-9\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-18\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-18\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-18\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-18\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-27\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-27\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-27\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-27\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-9] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-36\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-36\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-36\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-36\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-18] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-45\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-45\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-45\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-45\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-27] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-54\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-54\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-54\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-54\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-36] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-63\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-63\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-63\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-63\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-45] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-72\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-72\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-72\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-72\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-54] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-81\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-81\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-81\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-81\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-63] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-90\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-90\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-90\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-90\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-72] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./mistral-lora-itmoderno\\checkpoint-90 (score: 1.4490091800689697).\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-81] due to args.save_total_limit\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-finetuned-itmodern\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-finetuned-itmodern\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-finetuned-itmodern\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184bf8ee24cf4f98873b23edcd2c6957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Configuration saved in ./mistral-lora-merged\\config.json\n",
      "Configuration saved in ./mistral-lora-merged\\generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at ./mistral-lora-merged\\model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "tokenizer.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Carica il modello base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Carica il checkpoint LoRA appena salvato\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./mistral-finetuned-itmodern\")\n",
    "\n",
    "# Merge dei pesi LoRA nel modello base\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Salva il modello fuso come standalone\n",
    "merged_model.save_pretrained(\"./mistral-lora-merged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe729c",
   "metadata": {},
   "source": [
    "# üß† Traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efe421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file ./mistral-lora-merged\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Multi-backend validation successful.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file ./mistral-lora-merged\\model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Multi-backend validation successful.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563af5db61b1471ca5c1dd81c4a420e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ./mistral-lora-merged.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ./mistral-lora-merged\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 1/97 [00:05<09:05,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 2/97 [00:08<06:23,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 3/97 [00:11<05:28,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 4/97 [00:14<05:11,  3.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|‚ñå         | 5/97 [00:17<04:53,  3.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|‚ñå         | 6/97 [00:21<05:10,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|‚ñã         | 7/97 [00:26<05:54,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|‚ñä         | 8/97 [00:28<05:07,  3.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|‚ñâ         | 9/97 [00:36<07:02,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|‚ñà         | 10/97 [00:38<05:48,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|‚ñà‚ñè        | 11/97 [00:41<04:59,  3.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|‚ñà‚ñè        | 12/97 [00:45<05:27,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|‚ñà‚ñé        | 13/97 [00:48<05:00,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|‚ñà‚ñç        | 14/97 [00:51<04:47,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|‚ñà‚ñå        | 15/97 [00:55<04:43,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|‚ñà‚ñã        | 16/97 [00:59<05:07,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|‚ñà‚ñä        | 17/97 [01:02<04:40,  3.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|‚ñà‚ñä        | 18/97 [01:07<04:58,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|‚ñà‚ñâ        | 19/97 [01:15<06:32,  5.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|‚ñà‚ñà        | 20/97 [01:17<05:38,  4.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|‚ñà‚ñà‚ñè       | 21/97 [01:21<05:06,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|‚ñà‚ñà‚ñé       | 22/97 [01:23<04:32,  3.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|‚ñà‚ñà‚ñé       | 23/97 [01:25<03:49,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|‚ñà‚ñà‚ñç       | 24/97 [01:29<04:03,  3.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|‚ñà‚ñà‚ñå       | 25/97 [01:32<03:54,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|‚ñà‚ñà‚ñã       | 26/97 [01:35<03:42,  3.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|‚ñà‚ñà‚ñä       | 27/97 [01:40<04:08,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|‚ñà‚ñà‚ñâ       | 28/97 [01:43<04:06,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|‚ñà‚ñà‚ñâ       | 29/97 [01:51<05:36,  4.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|‚ñà‚ñà‚ñà       | 30/97 [01:56<05:30,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 31/97 [01:59<04:51,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 32/97 [02:03<04:26,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 33/97 [02:07<04:15,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 34/97 [02:09<03:45,  3.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 35/97 [02:16<04:40,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 36/97 [02:20<04:30,  4.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 37/97 [02:24<04:09,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 38/97 [02:26<03:27,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 39/97 [02:30<03:37,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 40/97 [02:32<03:07,  3.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 41/97 [02:37<03:30,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 42/97 [02:40<03:15,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 43/97 [02:43<02:56,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 44/97 [02:46<02:55,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 45/97 [02:48<02:32,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 46/97 [02:52<02:47,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 47/97 [02:56<02:43,  3.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 48/97 [02:59<02:48,  3.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 49/97 [03:01<02:23,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 50/97 [03:06<02:47,  3.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 51/97 [03:11<03:04,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 52/97 [03:14<02:48,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 53/97 [03:22<03:38,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 54/97 [03:26<03:15,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 55/97 [03:34<03:56,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 56/97 [03:37<03:13,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 57/97 [03:39<02:37,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 58/97 [03:41<02:12,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 59/97 [03:49<02:59,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 60/97 [03:56<03:27,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 61/97 [03:58<02:39,  4.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 62/97 [04:06<03:08,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 63/97 [04:13<03:25,  6.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 64/97 [04:17<02:58,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 65/97 [04:20<02:28,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 66/97 [04:22<02:01,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 67/97 [04:25<01:47,  3.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 68/97 [04:27<01:34,  3.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 69/97 [04:30<01:29,  3.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 70/97 [04:33<01:22,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 71/97 [04:36<01:20,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 72/97 [04:40<01:23,  3.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 73/97 [04:44<01:23,  3.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 74/97 [04:46<01:11,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 75/97 [04:54<01:40,  4.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 76/97 [04:57<01:25,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 77/97 [05:02<01:22,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 78/97 [05:06<01:18,  4.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 79/97 [05:13<01:33,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 80/97 [05:21<01:41,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 81/97 [05:24<01:22,  5.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 82/97 [05:28<01:11,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 83/97 [05:36<01:19,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84/97 [05:39<01:01,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85/97 [05:43<00:54,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 86/97 [05:46<00:45,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 87/97 [05:49<00:39,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 88/97 [05:54<00:36,  4.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 89/97 [05:56<00:28,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90/97 [05:58<00:21,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91/97 [06:01<00:17,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 92/97 [06:04<00:15,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 93/97 [06:07<00:11,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 94/97 [06:09<00:08,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 95/97 [06:12<00:05,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96/97 [06:17<00:03,  3.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [06:20<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 0. Quantization config (bnb4bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# 1. Seleziona il dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Carica il modello MERGED e tokenizer\n",
    "model_path = \"./mistral-lora-merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")  # tokenizer originale\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "# 3. Carica dataset\n",
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "df[\"generated_translation\"] = \"\"\n",
    "\n",
    "# 4. Funzione di traduzione\n",
    "def traduci(s):\n",
    "    prompt = getPrompt(s)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=nuovi_token_max,\n",
    "        temperature=temperatura,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Traduzione moderna:\")[-1].strip()\n",
    "\n",
    "# 5. Generazione con barra di avanzamento (solo primi 3 per test)\n",
    "results = []\n",
    "for i, s in enumerate(tqdm(df[\"Sentence\"].tolist())):\n",
    "    if max_translations!= 0 and i >= max_translations:\n",
    "        results.append(\"[SKIPPED]\")\n",
    "        continue\n",
    "    try:\n",
    "        results.append(traduci(s))\n",
    "    except:\n",
    "        results.append(\"[ERRORE]\")\n",
    "\n",
    "df[\"generated_translation\"] = results\n",
    "\n",
    "# 6. Salva i risultati\n",
    "df.to_csv(\"mnt/data/dataset_with_translations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Simulazione punteggi da LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scores = [5] * len(archaic_sentences)\n",
    "judge_scores_minerva = [5 if i % 3 != 0 else 4 for i in range(len(archaic_sentences))]\n",
    "judge_scores_llama = [4 if i % 2 == 0 else 5 for i in range(len(archaic_sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6771b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_minerva = cohen_kappa_score(manual_scores, judge_scores_minerva)\n",
    "kappa_llama = cohen_kappa_score(manual_scores, judge_scores_llama)\n",
    "\n",
    "print(f\"Cohen‚Äôs Kappa (Minerva): {kappa_minerva:.2f}\")\n",
    "print(f\"Cohen‚Äôs Kappa (LLaMA): {kappa_llama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e63bc",
   "metadata": {},
   "source": [
    "# üíæ Salvataggio risultati JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(\"groupX-hw2_transl-minerva350M\", archaic_sentences, translations_minerva)\n",
    "save_jsonl(\"groupX-hw2_transl-llama2_7b\", archaic_sentences, translations_llama)\n",
    "save_judging(\"groupX-hw2_transl-judge_minerva\", archaic_sentences, judge_scores_minerva)\n",
    "save_judging(\"groupX-hw2_transl-judge_llama\", archaic_sentences, judge_scores_llama)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197d4a",
   "metadata": {},
   "source": [
    "# üì• Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "archaic_sentences = df[\"Sentence\"].dropna().tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0ccd",
   "metadata": {},
   "source": [
    "# AUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0896026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Traduzioni\n",
    "def translate(pipe, sentences, label=\"Modello\"):\n",
    "    results = []\n",
    "    for s in sentences:\n",
    "        prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {s}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "        result = pipe(prompt, max_new_tokens=60, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
    "        trad = result.split(\"Traduzione moderna:\")[-1].strip().split(\"\\n\")[0]\n",
    "        print(f\"[{label}] ‚Üí\", trad)\n",
    "        results.append(trad)\n",
    "    return results\n",
    "\n",
    "# üíæ Salva risultati\n",
    "def save_jsonl(name, originals, translations):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for arc, trans in zip(originals, translations):\n",
    "            f.write(json.dumps({\"original\": arc, \"translation\": trans}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_judging(name, originals, scores):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (arc, score) in enumerate(zip(originals, scores)):\n",
    "            f.write(json.dumps({\"id\": i, \"original\": arc, \"score\": score}, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d432980",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento Minerva (su CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eded74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4032,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2048,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at sapienzanlp/Minerva-350M-base-v1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "minerva_name = \"sapienzanlp/Minerva-350M-base-v1.0\"\n",
    "minerva_tokenizer = AutoTokenizer.from_pretrained(minerva_name)\n",
    "minerva_model = AutoModelForCausalLM.from_pretrained(minerva_name)\n",
    "minerva_pipe = pipeline(\"text-generation\", model=minerva_model, tokenizer=minerva_tokenizer, device= -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be4176",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento LLaMA-2 7B (su GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817c4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52f30cb60404872a2823f61f24666ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "#llama_name = \"meta-llama/Llama-3-8b-hf\" llama 3.1 1B\n",
    "llama_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "llama_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f795e7",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf028d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b045c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"fine_tuning/dataset_dante_purgat.csv\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94a08",
   "metadata": {},
   "source": [
    "## Modello e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39718bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c731dbb4c441b8ada922b32bbeb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Per evitare errori su padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6b9",
   "metadata": {},
   "source": [
    "## Configurazione Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4322b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a6df",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca686f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa58fc40fc44fc4802f7c3228ae8715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7846439207c4d87b7d3780b27d625f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {example['text']}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"translation\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(format_prompt, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"test\": dataset[\"test\"].map(format_prompt, remove_columns=dataset[\"test\"].column_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb0fe",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510db0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\colam\\AppData\\Local\\Temp\\ipykernel_31136\\1455714567.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, logging\n",
    "logging.set_verbosity_debug()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lora-itmoderno\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,        # ‚úÖ abilita tqdm\n",
    "    report_to=\"none\",          # evita warning da WandB o altri\n",
    "    logging_dir=\"./logs\",      # facoltativo\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "def compute_metrics(eval_preds):\n",
    "    return {} \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9d74",
   "metadata": {},
   "source": [
    "## Avvia Fine Tuning & Salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7560514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 36\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 15\n",
      "  Number of trainable parameters = 3,407,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.059828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.484354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.375222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.777906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.504958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-3\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-3\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-3\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-3\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-6\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-6\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-6\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-15] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-9\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-9\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-9\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-9\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-3] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-12\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-12\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-12\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-12\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-6] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-15\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-15\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-15\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-15\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-9] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./mistral-lora-itmoderno\\checkpoint-15 (score: 2.504958152770996).\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-12] due to args.save_total_limit\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "chat template saved in ./mistral-finetuned-itmodern\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-finetuned-itmodern\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-finetuned-itmodern\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./mistral-finetuned-itmodern\\\\tokenizer_config.json',\n",
       " './mistral-finetuned-itmodern\\\\special_tokens_map.json',\n",
       " './mistral-finetuned-itmodern\\\\chat_template.jinja',\n",
       " './mistral-finetuned-itmodern\\\\tokenizer.model',\n",
       " './mistral-finetuned-itmodern\\\\added_tokens.json',\n",
       " './mistral-finetuned-itmodern\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "tokenizer.save_pretrained(\"./mistral-finetuned-itmodern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe729c",
   "metadata": {},
   "source": [
    "# üß† Traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efe421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#translations_minerva = translate(minerva_pipe, archaic_sentences, label=\"Minerva\")\n",
    "#translations_llama = translate(llama_pipe, archaic_sentences, label=\"LLaMA\")\n",
    "\n",
    "# Seleziona il dispositivo\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "model_path = \"./mistral-finetuned-itmodern\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval().to(device)\n",
    "\n",
    "# 2. Load dataset\n",
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "df[\"generated_translation\"] = \"\"\n",
    "\n",
    "# 3. Define translation function\n",
    "def traduci(s):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {s}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Traduzione moderna:\")[-1].strip()\n",
    "\n",
    "# 4. Process with progress bar\n",
    "results = []\n",
    "i =0\n",
    "for s in tqdm(df[\"Sentence\"].tolist()):\n",
    "    i+=1\n",
    "    if(i>3):\n",
    "        results.append(\"[SKIPPED]\")\n",
    "        continue\n",
    "    try:\n",
    "        results.append(traduci(s))\n",
    "    except:\n",
    "        results.append(\"[ERRORE]\")\n",
    "        \n",
    "df[\"generated_translation\"] = results\n",
    "\n",
    "# 5. Save result\n",
    "df.to_csv(\"mnt/data/dataset_with_translations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Simulazione punteggi da LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81cd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scores = [5] * len(archaic_sentences)\n",
    "judge_scores_minerva = [5 if i % 3 != 0 else 4 for i in range(len(archaic_sentences))]\n",
    "judge_scores_llama = [4 if i % 2 == 0 else 5 for i in range(len(archaic_sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6771b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen‚Äôs Kappa (Minerva): 0.00\n",
      "Cohen‚Äôs Kappa (LLaMA): 0.00\n"
     ]
    }
   ],
   "source": [
    "kappa_minerva = cohen_kappa_score(manual_scores, judge_scores_minerva)\n",
    "kappa_llama = cohen_kappa_score(manual_scores, judge_scores_llama)\n",
    "\n",
    "print(f\"Cohen‚Äôs Kappa (Minerva): {kappa_minerva:.2f}\")\n",
    "print(f\"Cohen‚Äôs Kappa (LLaMA): {kappa_llama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e63bc",
   "metadata": {},
   "source": [
    "# üíæ Salvataggio risultati JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9004b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(\"groupX-hw2_transl-minerva350M\", archaic_sentences, translations_minerva)\n",
    "save_jsonl(\"groupX-hw2_transl-llama2_7b\", archaic_sentences, translations_llama)\n",
    "save_judging(\"groupX-hw2_transl-judge_minerva\", archaic_sentences, judge_scores_minerva)\n",
    "save_judging(\"groupX-hw2_transl-judge_llama\", archaic_sentences, judge_scores_llama)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

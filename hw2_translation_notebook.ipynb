{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197d4a",
   "metadata": {},
   "source": [
    "# üì• Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "archaic_sentences = df[\"Sentence\"].dropna().tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0ccd",
   "metadata": {},
   "source": [
    "# AUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0896026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Traduzioni\n",
    "def translate(pipe, sentences, label=\"Modello\"):\n",
    "    results = []\n",
    "    for s in sentences:\n",
    "        prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {s}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "        result = pipe(prompt, max_new_tokens=60, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
    "        trad = result.split(\"Traduzione moderna:\")[-1].strip().split(\"\\n\")[0]\n",
    "        print(f\"[{label}] ‚Üí\", trad)\n",
    "        results.append(trad)\n",
    "    return results\n",
    "\n",
    "# üíæ Salva risultati\n",
    "def save_jsonl(name, originals, translations):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for arc, trans in zip(originals, translations):\n",
    "            f.write(json.dumps({\"original\": arc, \"translation\": trans}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_judging(name, originals, scores):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (arc, score) in enumerate(zip(originals, scores)):\n",
    "            f.write(json.dumps({\"id\": i, \"original\": arc, \"score\": score}, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d432980",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento Minerva (su CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eded74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "minerva_name = \"sapienzanlp/Minerva-350M-base-v1.0\"\n",
    "minerva_tokenizer = AutoTokenizer.from_pretrained(minerva_name)\n",
    "minerva_model = AutoModelForCausalLM.from_pretrained(minerva_name)\n",
    "minerva_pipe = pipeline(\"text-generation\", model=minerva_model, tokenizer=minerva_tokenizer, device= -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be4176",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento LLaMA-2 7B (su GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817c4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52f30cb60404872a2823f61f24666ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "#llama_name = \"meta-llama/Llama-3-8b-hf\" llama 3.1 1B\n",
    "llama_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "llama_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f795e7",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf028d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b045c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"fine_tuning/dataset_dante_purgat.csv\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94a08",
   "metadata": {},
   "source": [
    "## Modello e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39718bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b7ccd100fa461798a8895dc9216b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Per evitare errori su padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6b9",
   "metadata": {},
   "source": [
    "## Configurazione Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4322b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a6df",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca686f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc36f75d84c048bca4464c7afabb7641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61a67e17cd44320870fae6104c28d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {example['text']}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"translation\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(format_prompt, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"test\": dataset[\"test\"].map(format_prompt, remove_columns=dataset[\"test\"].column_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb0fe",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510db0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\colam\\AppData\\Local\\Temp\\ipykernel_28220\\1369321324.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, logging\n",
    "logging.set_verbosity_debug()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lora-itmoderno\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,        # ‚úÖ abilita tqdm\n",
    "    report_to=\"none\",          # evita warning da WandB o altri\n",
    "    logging_dir=\"./logs\",      # facoltativo\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "def compute_metrics(eval_preds):\n",
    "    return {} \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9d74",
   "metadata": {},
   "source": [
    "## Avvia Fine Tuning & Salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7560514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 36\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 9\n",
      "  Number of trainable parameters = 3,407,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/9 00:08 < 00:22, 0.22 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3164\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3164\u001b[39m     metric_value = \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mKeyError\u001b[39m: 'eval_loss'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./mistral-finetuned-itmodern\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./mistral-finetuned-itmodern\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2656\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2653\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2656\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2662\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3096\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m   3095\u001b[39m     metrics = \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval)\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m     is_new_best_metric = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_best_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3098\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n\u001b[32m   3099\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3166\u001b[39m, in \u001b[36mTrainer._determine_best_metric\u001b[39m\u001b[34m(self, metrics, trial)\u001b[39m\n\u001b[32m   3164\u001b[39m     metric_value = metrics[metric_to_check]\n\u001b[32m   3165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m-> \u001b[39m\u001b[32m3166\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   3167\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, which is not found in the evaluation metrics. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3168\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3169\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   3171\u001b[39m operator = np.greater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.greater_is_better \u001b[38;5;28;01melse\u001b[39;00m np.less\n\u001b[32m   3173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "tokenizer.save_pretrained(\"./mistral-finetuned-itmodern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe729c",
   "metadata": {},
   "source": [
    "# üß† Traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c940a8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí il termine fu inventato nel 1859 dal francese Bernard Joulaye, che aveva scritto un trattato di traduzione in inglese.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí non √® vero che i traduttori devono tradurre tutti i testi antichi, ma non tutti, e soprattutto non tutti, che la traduzione sia \"migliore\" o \"pi√π bella\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí La traduzione √® l'atto di tradurre in italiano, l'atto di tradurre in italiano, la traduzione √® il mezzo di tradurre in italiano.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Se questo piace a tutti e se 'l tempo hae bisogno di traduttore, non riterr√≤ pi√π i fati.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí √à un po' come il nome del tuo nome, perch√© √® un nome di famiglia.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí √à il pi√π antico testo antico, ma l'ho scritto io stesso; il che mi ha fatto pensare che il testo antico fosse un'opera d'arte, di cui gli scrittori antichi non conoscevano il senso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Ma chi avrebbe mai pensato che la lingua di Cristo, la sua lingua, sarebbe la lingua di Cristo?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí In questa lingua, la lingua della quale si parla, √® la lingua latina, l'italiano e il francese.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Con i suoi tempi, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo, e di modo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí La verit√† √® una delle pi√π antiche.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Marco Cornelio, o pi√π semplicemente Marco Cornelio, si √® distinto per aver studiato in un anno accademico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí parole che erano fatte in Italia.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Ildegarda di Bingen, il figlio di Ortensio, che era stato nominato per questo compito, fu chiamato da Dio e fu chiamato da Dio a servire il suo Dio in tutto.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Il traduttore inglese usa l'alfabeto arabo e l'alfabeto greco.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Io mi ricordo (ch. 352) che non ricordo (ch. 361) che non ricordo (ch. 363) che non ricordo (ch. 365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí colui del quale tu ti tti dolere ch' avevi amato;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí Ma non vi √® alcun dubbio che la donna non era la madre di tutti i figli.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí O ti dixe: \"O fedele mia donna,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Minerva] ‚Üí √à di origine antica.\n",
      "[Minerva] ‚Üí E sibillina la mia mente, e la mia fede, e la mia speranza, e il mio buon proposito.\n",
      "[LLaMA] ‚Üí quella guerra ben fatta l' opera perch√© etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno\n",
      "[LLaMA] ‚Üí crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a nessuno cavaliere perdoni che pecchi.\n",
      "[LLaMA] ‚Üí Di altre forze d' animo non fu ornato Ponzio Aufidiano, cavaliere romano.\n",
      "[LLaMA] ‚Üí Se questo piace a tutti e se il tempo ha bisogno d'aver Pompeio per cavaliere e non per compagno, non riterr√≤ pi√π i fati.\n",
      "[LLaMA] ‚Üí L'arte di questa professione √® di dire cose che non sono vere, per far credere che le cose che dicono siano vere.\n",
      "[LLaMA] ‚Üí Ecco e larghi ventipiovoli caggiono delle nebbie risolute; e potresti credere che tutto il cielo cadesse nel mare\n",
      "[LLaMA] ‚Üí Ma che se alcuni che non ancora credono in Cristo, pur vedendo con noi ci√≤ che non possono negare, gridano di dolore.\n",
      "[LLaMA] ‚Üí Il re furioso fece le vendite dei morti e la pressione dei vivi.\n",
      "[LLaMA] ‚Üí Perch√© quel giovane, che ora per i suoi grandi peccati √® feroce e onorato, egli afflitto e tormentato dalla disubbidienza verso il mio padre.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLaMA] ‚Üí Quanto pi√π grande √® l'anima\n",
      "[LLaMA] ‚Üí Marco Cornelio, uno dei dieci compagni, riservava di parlare all'ultimo.\n",
      "[LLaMA] ‚Üí cose che io sapeva che erano fatte in Italia.\n",
      "[LLaMA] ‚Üí Velleio, figlio di L\n",
      "[LLaMA] ‚Üí Inoltre uno amante chiamando la sua donna merz√© dice parole e ragioni molte, et ella si difende in suo dire.\n",
      "[LLaMA] ‚Üí Io ricordo che una volta, per sfogare la mia ira, mi scapigliai la donna. Ohi, quanti giorni di ira m'ha tolto!\n",
      "[LLaMA] ‚Üí di cui ti facevi pena,\n",
      "[LLaMA] ‚Üí Ma non sapevano ancora le nomora del congiurato, perch√© la donna non nominava ancora i nomi.\n",
      "[LLaMA] ‚Üí Creti? Certo, quando lui mosse, disse: \"O fedele mia donna, facci che tu sia accolta come un'ospite in casa del nostro ospite troiano.\"\n",
      "[LLaMA] ‚Üí A Milano fu repressa la malvagit√† di una donna in simile bug√¨a, nel tempo medesimo di questo signore della repubblica, in questo modo:\n",
      "[LLaMA] ‚Üí E lamentavansi dell'iniquit√† d'Appio, e ripiagnevano la malavventurata belt√† della pulcella e la necessit√† del padre.\n"
     ]
    }
   ],
   "source": [
    "translations_minerva = translate(minerva_pipe, archaic_sentences, label=\"Minerva\")\n",
    "translations_llama = translate(llama_pipe, archaic_sentences, label=\"LLaMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Simulazione punteggi da LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81cd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scores = [5] * len(archaic_sentences)\n",
    "judge_scores_minerva = [5 if i % 3 != 0 else 4 for i in range(len(archaic_sentences))]\n",
    "judge_scores_llama = [4 if i % 2 == 0 else 5 for i in range(len(archaic_sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6771b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen‚Äôs Kappa (Minerva): 0.00\n",
      "Cohen‚Äôs Kappa (LLaMA): 0.00\n"
     ]
    }
   ],
   "source": [
    "kappa_minerva = cohen_kappa_score(manual_scores, judge_scores_minerva)\n",
    "kappa_llama = cohen_kappa_score(manual_scores, judge_scores_llama)\n",
    "\n",
    "print(f\"Cohen‚Äôs Kappa (Minerva): {kappa_minerva:.2f}\")\n",
    "print(f\"Cohen‚Äôs Kappa (LLaMA): {kappa_llama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e63bc",
   "metadata": {},
   "source": [
    "# üíæ Salvataggio risultati JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9004b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(\"groupX-hw2_transl-minerva350M\", archaic_sentences, translations_minerva)\n",
    "save_jsonl(\"groupX-hw2_transl-llama2_7b\", archaic_sentences, translations_llama)\n",
    "save_judging(\"groupX-hw2_transl-judge_minerva\", archaic_sentences, judge_scores_minerva)\n",
    "save_judging(\"groupX-hw2_transl-judge_llama\", archaic_sentences, judge_scores_llama)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

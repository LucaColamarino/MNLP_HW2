{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, logging\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197d4a",
   "metadata": {},
   "source": [
    "# üì• Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8974cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"inputs/dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "archaic_sentences = df[\"Sentence\"].dropna().tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f795e7",
   "metadata": {},
   "source": [
    "# üë®‚Äçüè´ Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26658fb1",
   "metadata": {},
   "source": [
    "## Parametri fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Percorso alla cartella con i file CSV\n",
    "cartella_csvs = \"fine_tuning/csvs\"  \n",
    "# üìÅ Percorso alla cartella dove andr√† il dataset concatenato\n",
    "cartella_dataset_concatenato = \"inputs/dataset_concatenato.csv\"\n",
    "\n",
    "training_epochs=15\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#model_name = \"bigscience/bloomz-3b\"\n",
    "\n",
    "nuovi_token_max=100\n",
    "temperatura=0.7\n",
    "max_translations=0 # 0 = no limit, >0 = max number of translations to generate\n",
    "\n",
    "def getPrompt(archaic_sentence):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {archaic_sentence}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf028d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 296 examples [00:00, 29491.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# üîç Prende tutti i file .csv nella cartella\n",
    "csv_files = glob.glob(os.path.join(cartella_csvs, \"*.csv\"))\n",
    "\n",
    "# üì¶ Carica e concatena tutti i file\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel file {file}: {e}\")\n",
    "\n",
    "# üìö Unione verticale\n",
    "df_finale = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# üíæ Salvataggio\n",
    "df_finale.to_csv(cartella_dataset_concatenato, index=False)\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=cartella_dataset_concatenato)[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94a08",
   "metadata": {},
   "source": [
    "## Modello e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39718bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True) #use_fast=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Per evitare errori su padding\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.is_loaded_in_4bit = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6b9",
   "metadata": {},
   "source": [
    "## Configurazione Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4322b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a6df",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca686f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 266/266 [00:00<00:00, 921.21 examples/s] \n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 766.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = getPrompt(example['text'])\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"translation\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(format_prompt, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"test\": dataset[\"test\"].map(format_prompt, remove_columns=dataset[\"test\"].column_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb0fe",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510db0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\colam\\AppData\\Local\\Temp\\ipykernel_22392\\1120740903.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_debug()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Modelli/Mistral/mistral-lora-itmoderno\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=training_epochs,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,        # ‚úÖ abilita tqdm\n",
    "    report_to=\"none\",          # evita warning da WandB o altri\n",
    "    logging_dir=\"./logs\",      # facoltativo\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "def compute_metrics(eval_preds):\n",
    "    return {} \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9d74",
   "metadata": {},
   "source": [
    "## Avvia Fine Tuning & Salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 1\n",
      "***** Running training *****\n",
      "  Num examples = 266\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 255\n",
      "  Number of trainable parameters = 3,407,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 46:28, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.876412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.831600</td>\n",
       "      <td>1.597599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.668000</td>\n",
       "      <td>1.566959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.507000</td>\n",
       "      <td>1.563998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.393100</td>\n",
       "      <td>1.567755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>1.605215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>1.637010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.143000</td>\n",
       "      <td>1.763938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>1.750238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>1.883890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>1.918193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>1.993713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>2.029191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>2.101727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>2.141549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-17\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-17\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-17\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-17\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-34\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-34\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-34\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-34\\special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-51\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-51\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-51\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-51\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-17] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-68\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-68\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-68\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-68\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-34] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-85\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-85\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-85\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-85\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-51] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-102\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-102\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-102\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-102\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-85] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-119\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-119\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-119\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-119\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-102] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-136\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-136\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-136\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-136\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-119] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-153\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-153\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-153\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-153\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-136] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-170\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-170\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-170\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-170\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-153] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-187\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-187\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-187\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-187\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-170] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-204\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-204\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-204\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-204\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-187] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-221\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-221\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-221\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-221\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-204] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-238\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-238\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-238\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-238\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-221] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./mistral-lora-itmoderno\\checkpoint-255\n",
      "chat template saved in ./mistral-lora-itmoderno\\checkpoint-255\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-lora-itmoderno\\checkpoint-255\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-lora-itmoderno\\checkpoint-255\\special_tokens_map.json\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-238] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./mistral-lora-itmoderno\\checkpoint-68 (score: 1.5639984607696533).\n",
      "Deleting older checkpoint [mistral-lora-itmoderno\\checkpoint-255] due to args.save_total_limit\n",
      "chat template saved in ./mistral-finetuned-itmodern\\chat_template.jinja\n",
      "tokenizer config file saved in ./mistral-finetuned-itmodern\\tokenizer_config.json\n",
      "Special tokens file saved in ./mistral-finetuned-itmodern\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:43<00:00, 14.61s/it]\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Configuration saved in ./mistral-lora-merged\\config.json\n",
      "Configuration saved in ./mistral-lora-merged\\generation_config.json\n",
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at ./mistral-lora-merged\\model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"Modelli/Mistral/mistral-finetuned-itmodern\")\n",
    "tokenizer.save_pretrained(\"Modelli/Mistral/mistral-finetuned-itmodern\")\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Carica il modello base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Carica il checkpoint LoRA appena salvato\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"Modelli/Mistral/mistral-finetuned-itmodern\")\n",
    "\n",
    "# Merge dei pesi LoRA nel modello base\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Salva il modello fuso come standalone\n",
    "merged_model.save_pretrained(\"Modelli/Mistral/mistral-lora-merged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe729c",
   "metadata": {},
   "source": [
    "# üß† Traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efe421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file ./mistral-lora-merged\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Multi-backend validation successful.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file ./mistral-lora-merged\\model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Multi-backend validation successful.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:43<00:00,  7.21s/it]\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ./mistral-lora-merged.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ./mistral-lora-merged\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 1/97 [00:06<10:03,  6.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 2/97 [00:10<07:58,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|‚ñé         | 3/97 [00:13<06:36,  4.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|‚ñç         | 4/97 [00:24<10:28,  6.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|‚ñå         | 5/97 [00:27<08:31,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|‚ñå         | 6/97 [00:32<08:07,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|‚ñã         | 7/97 [00:37<07:53,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|‚ñä         | 8/97 [00:40<06:41,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|‚ñâ         | 9/97 [00:46<07:14,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|‚ñà         | 10/97 [00:49<06:06,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|‚ñà‚ñè        | 11/97 [00:52<05:27,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|‚ñà‚ñè        | 12/97 [01:01<08:00,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|‚ñà‚ñé        | 13/97 [01:05<06:49,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|‚ñà‚ñç        | 14/97 [01:09<06:25,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|‚ñà‚ñå        | 15/97 [01:12<05:56,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|‚ñà‚ñã        | 16/97 [01:15<05:15,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|‚ñà‚ñä        | 17/97 [01:18<04:57,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|‚ñà‚ñä        | 18/97 [01:23<05:18,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|‚ñà‚ñâ        | 19/97 [01:34<07:51,  6.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|‚ñà‚ñà        | 20/97 [01:38<06:58,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|‚ñà‚ñà‚ñè       | 21/97 [01:42<06:18,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|‚ñà‚ñà‚ñé       | 22/97 [01:45<05:33,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|‚ñà‚ñà‚ñé       | 23/97 [01:47<04:44,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|‚ñà‚ñà‚ñç       | 24/97 [01:53<05:10,  4.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|‚ñà‚ñà‚ñå       | 25/97 [01:56<04:55,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|‚ñà‚ñà‚ñã       | 26/97 [01:58<03:55,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|‚ñà‚ñà‚ñä       | 27/97 [02:02<04:14,  3.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|‚ñà‚ñà‚ñâ       | 28/97 [02:07<04:31,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|‚ñà‚ñà‚ñâ       | 29/97 [02:13<05:16,  4.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|‚ñà‚ñà‚ñà       | 30/97 [02:20<05:51,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 31/97 [02:25<05:42,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 32/97 [02:28<04:56,  4.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 33/97 [02:32<04:38,  4.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 34/97 [02:42<06:31,  6.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 35/97 [02:46<05:30,  5.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 36/97 [02:56<07:00,  6.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 37/97 [03:00<05:56,  5.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 38/97 [03:03<04:52,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 39/97 [03:08<04:52,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 40/97 [03:15<05:26,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 41/97 [03:23<05:46,  6.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 42/97 [03:28<05:21,  5.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 43/97 [03:31<04:35,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 44/97 [03:42<06:01,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 45/97 [03:44<04:43,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 46/97 [03:50<04:40,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 47/97 [03:54<04:19,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 48/97 [03:59<04:16,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 49/97 [04:02<03:35,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 50/97 [04:08<03:45,  4.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 51/97 [04:14<03:56,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 52/97 [04:18<03:37,  4.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 53/97 [04:20<02:57,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 54/97 [04:25<03:11,  4.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 55/97 [04:36<04:22,  6.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 56/97 [04:39<03:42,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 57/97 [04:42<03:05,  4.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 58/97 [04:55<04:39,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 59/97 [05:00<04:01,  6.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 60/97 [05:12<05:06,  8.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 61/97 [05:16<04:06,  6.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 62/97 [05:19<03:20,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 63/97 [05:24<03:08,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 64/97 [05:31<03:19,  6.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 65/97 [05:35<02:52,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 66/97 [05:38<02:26,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 67/97 [05:42<02:15,  4.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 68/97 [05:46<02:00,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 69/97 [05:51<02:05,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 70/97 [05:55<01:59,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 71/97 [05:59<01:51,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 72/97 [06:08<02:17,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 73/97 [06:15<02:28,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 74/97 [06:19<02:06,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 75/97 [06:27<02:17,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 76/97 [06:32<02:03,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 77/97 [06:39<02:04,  6.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 78/97 [06:45<01:55,  6.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 79/97 [06:49<01:37,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 80/97 [06:56<01:42,  6.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 81/97 [07:01<01:29,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 82/97 [07:07<01:26,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 83/97 [07:13<01:20,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 84/97 [07:17<01:10,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 85/97 [07:23<01:04,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 86/97 [07:28<00:58,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 87/97 [07:33<00:53,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 88/97 [07:41<00:54,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 89/97 [07:44<00:42,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 90/97 [07:47<00:32,  4.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 91/97 [07:52<00:27,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 92/97 [07:57<00:23,  4.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 93/97 [08:00<00:17,  4.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 94/97 [08:12<00:19,  6.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 95/97 [08:16<00:11,  5.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 96/97 [08:23<00:06,  6.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [08:25<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 0. Quantization config (bnb4bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# 1. Seleziona il dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Carica il modello MERGED e tokenizer\n",
    "model_path = \"Modelli/Mistral/mistral-lora-merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")  # tokenizer originale\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "# 3. Carica dataset\n",
    "df = pd.read_csv(\"inputs/dataset.csv\")\n",
    "df[\"generated_translation\"] = \"\"\n",
    "\n",
    "# 4. Funzione di traduzione\n",
    "def traduci(s):\n",
    "    prompt = getPrompt(s)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=nuovi_token_max,\n",
    "        temperature=temperatura,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Traduzione moderna:\")[-1].strip()\n",
    "\n",
    "# 5. Generazione con barra di avanzamento (solo primi 3 per test)\n",
    "results = []\n",
    "for i, s in enumerate(tqdm(df[\"Sentence\"].tolist())):\n",
    "    if max_translations!= 0 and i >= max_translations:\n",
    "        results.append(\"[SKIPPED]\")\n",
    "        continue\n",
    "    try:\n",
    "        results.append(traduci(s))\n",
    "    except:\n",
    "        results.append(\"[ERRORE]\")\n",
    "\n",
    "df[\"generated_translation\"] = results\n",
    "\n",
    "# 6. Salva i risultati\n",
    "df.to_csv(\"outputs/dataset_with_mistral_translations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Prometheus LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0448b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:08<00:00,  1.08s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "prometheur_model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "\n",
    "prometheus_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    prometheur_model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "prometheus_model = AutoModelForCausalLM.from_pretrained(\n",
    "    prometheur_model_name,\n",
    "    trust_remote_code=True,         # <-- importante anche qui\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"offload_prometheus\"\n",
    ")\n",
    "\n",
    "judge = pipeline(\"text-generation\", model=prometheus_model, tokenizer=prometheus_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i dataset\n",
    "df_prometheus = pd.read_csv(\"inputs/dataset_human_eval.csv\")\n",
    "df_translations = pd.read_csv(\"outputs/dataset_with_mistral_translations.csv\")\n",
    "\n",
    "# Usa solo i primi 15 elementi\n",
    "df_prometheus = df_prometheus.head(15)\n",
    "df_translations = df_translations.head(15)\n",
    "\n",
    "# Aggiunge la colonna 'generated_translation'\n",
    "df_prometheus[\"generated_translation\"] = df_translations[\"generated_translation\"]\n",
    "\n",
    "# Funzione per costruire il prompt\n",
    "def build_judge_prompt(original, human_translation, model_translation):\n",
    "    return f\"\"\"\n",
    "You are a translation evaluator.\n",
    "\n",
    "Original:\n",
    "{original}\n",
    "\n",
    "Human:\n",
    "{human_translation}\n",
    "\n",
    "Model:\n",
    "{model_translation}\n",
    "\n",
    "Score (1‚Äì5):\n",
    "\"\"\".strip()\n",
    "\n",
    "# Esegui judge su ogni riga e salva il risultato\n",
    "scores = []\n",
    "for i, row in df_prometheus.iterrows():\n",
    "    prompt = build_judge_prompt(row[\"Sentence\"], row[\"HumanEval\"], row[\"generated_translation\"])\n",
    "    result = judge(prompt, max_new_tokens=2, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "    scores.append(result.strip())\n",
    "\n",
    "# Aggiunge la colonna dei punteggi\n",
    "df_prometheus[\"judge_score\"] = scores\n",
    "\n",
    "# Salva il risultato\n",
    "df_prometheus.to_csv(\"outputs/dataset_with_evaluation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa49fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a translation evaluator.\\n\\nOriginal:\\nAd te solo, Altissimo, se konfano, et nullu homo √®ne dignu te mentovare.\\n\\nHuman:\\nA te solo, altissimo si addicono e nessun uomo √® degno di menzionare il tuo nome.\\n\\nModel:\\nSolo a te, sommo Signore, spettano gli onori, e nessuno √® degno di nominarti.\\n\\nScore (1‚Äì5): 5\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e63bc",
   "metadata": {},
   "source": [
    "# üíæ Salvataggio risultati JSONL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

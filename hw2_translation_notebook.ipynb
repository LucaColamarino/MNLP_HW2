{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af197d4a",
   "metadata": {},
   "source": [
    "# üì• Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cf37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "archaic_sentences = df[\"Sentence\"].dropna().tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d0ccd",
   "metadata": {},
   "source": [
    "# AUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0896026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Traduzioni\n",
    "def translate(pipe, sentences, label=\"Modello\"):\n",
    "    results = []\n",
    "    for s in sentences:\n",
    "        prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {s}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "        result = pipe(prompt, max_new_tokens=60, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
    "        trad = result.split(\"Traduzione moderna:\")[-1].strip().split(\"\\n\")[0]\n",
    "        print(f\"[{label}] ‚Üí\", trad)\n",
    "        results.append(trad)\n",
    "    return results\n",
    "\n",
    "# üíæ Salva risultati\n",
    "def save_jsonl(name, originals, translations):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for arc, trans in zip(originals, translations):\n",
    "            f.write(json.dumps({\"original\": arc, \"translation\": trans}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_judging(name, originals, scores):\n",
    "    with open(f\"mnt/data/{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (arc, score) in enumerate(zip(originals, scores)):\n",
    "            f.write(json.dumps({\"id\": i, \"original\": arc, \"score\": score}, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d432980",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento Minerva (su CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eded74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1152,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4032,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2048,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at sapienzanlp/Minerva-350M-base-v1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--sapienzanlp--Minerva-350M-base-v1.0\\snapshots\\65e78a1551d0ea16f3f5860e846b8bd6b5e9a59b\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "minerva_name = \"sapienzanlp/Minerva-350M-base-v1.0\"\n",
    "minerva_tokenizer = AutoTokenizer.from_pretrained(minerva_name)\n",
    "minerva_model = AutoModelForCausalLM.from_pretrained(minerva_name)\n",
    "minerva_pipe = pipeline(\"text-generation\", model=minerva_model, tokenizer=minerva_tokenizer, device= -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be4176",
   "metadata": {},
   "source": [
    "# ‚úÖ Caricamento LLaMA-2 7B (su GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama_name = \"meta-llama/Llama-3-8b-hf\" llama 3.1 1B\n",
    "llama_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_name)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "llama_pipe = pipeline(\"text-generation\", model=llama_model, tokenizer=llama_tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f795e7",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf028d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b045c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"fine_tuning/dataset_dante_purgat.csv\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd94a08",
   "metadata": {},
   "source": [
    "## Modello e Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39718bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527ab775e31e4166a20ade8f87083f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#model_name = \"bigscience/bloomz-3b\"\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Per evitare errori su padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f9f6b9",
   "metadata": {},
   "source": [
    "## Configurazione Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4322b8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1a6df",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca686f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31db5c05985a45b0b7dce68183e72a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7732c442163424d8ff826acd7576598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {example['text']}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "        )\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(example[\"translation\"], truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(format_prompt, remove_columns=dataset[\"train\"].column_names),\n",
    "    \"test\": dataset[\"test\"].map(format_prompt, remove_columns=dataset[\"test\"].column_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bb0fe",
   "metadata": {},
   "source": [
    "## Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "510db0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\colam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "C:\\Users\\colam\\AppData\\Local\\Temp\\ipykernel_13100\\3884762455.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, logging\n",
    "logging.set_verbosity_debug()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-lora-itmoderno\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,        # ‚úÖ abilita tqdm\n",
    "    report_to=\"none\",          # evita warning da WandB o altri\n",
    "    logging_dir=\"./logs\",      # facoltativo\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "def compute_metrics(eval_preds):\n",
    "    return {} \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9d74",
   "metadata": {},
   "source": [
    "## Avvia Fine Tuning & Salva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7560514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at ./mistral-lora-merged\\model.safetensors.index.json.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "tokenizer.save_pretrained(\"./mistral-finetuned-itmodern\")\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Carica il modello base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Carica il checkpoint LoRA appena salvato\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./mistral-finetuned-itmodern\")\n",
    "\n",
    "# Merge dei pesi LoRA nel modello base\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Salva il modello fuso come standalone\n",
    "merged_model.save_pretrained(\"./mistral-lora-merged\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84850d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15fe729c",
   "metadata": {},
   "source": [
    "# üß† Traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efe421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\colam\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2\\snapshots\\3ad372fc79158a2148299e3318516c786aeded6c\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file ./mistral-lora-merged\\config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Multi-backend validation successful.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file ./mistral-lora-merged\\model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Multi-backend validation successful.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb770bc04e4433b819f82883bc1a6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at ./mistral-lora-merged.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file ./mistral-lora-merged\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  1%|          | 1/97 [00:04<07:06,  4.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  2%|‚ñè         | 2/97 [00:07<05:33,  3.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:15<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 0. Quantization config (bnb4bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# 1. Seleziona il dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Carica il modello MERGED e tokenizer\n",
    "model_path = \"./mistral-lora-merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")  # tokenizer originale\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "# 3. Carica dataset\n",
    "df = pd.read_csv(\"mnt/data/dataset_cleaned.csv\")\n",
    "df[\"generated_translation\"] = \"\"\n",
    "\n",
    "# 4. Funzione di traduzione\n",
    "def traduci(s):\n",
    "    prompt = (\n",
    "        \"Sei un traduttore professionista di testi antichi in italiano moderno.\\n\"\n",
    "        \"Trasforma la seguente frase antica in italiano moderno, mantenendo il significato.\\n\"\n",
    "        f\"Testo antico: {s}\\n\"\n",
    "        \"Traduzione moderna:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"Traduzione moderna:\")[-1].strip()\n",
    "\n",
    "# 5. Generazione con barra di avanzamento (solo primi 3 per test)\n",
    "results = []\n",
    "for i, s in enumerate(tqdm(df[\"Sentence\"].tolist())):\n",
    "    #if i >= 3:\n",
    "    #    results.append(\"[SKIPPED]\")\n",
    "    #    continue\n",
    "    try:\n",
    "        results.append(traduci(s))\n",
    "    except:\n",
    "        results.append(\"[ERRORE]\")\n",
    "\n",
    "df[\"generated_translation\"] = results\n",
    "\n",
    "# 6. Salva i risultati\n",
    "df.to_csv(\"mnt/data/dataset_with_translations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Simulazione punteggi da LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81cd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_scores = [5] * len(archaic_sentences)\n",
    "judge_scores_minerva = [5 if i % 3 != 0 else 4 for i in range(len(archaic_sentences))]\n",
    "judge_scores_llama = [4 if i % 2 == 0 else 5 for i in range(len(archaic_sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6771b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen‚Äôs Kappa (Minerva): 0.00\n",
      "Cohen‚Äôs Kappa (LLaMA): 0.00\n"
     ]
    }
   ],
   "source": [
    "kappa_minerva = cohen_kappa_score(manual_scores, judge_scores_minerva)\n",
    "kappa_llama = cohen_kappa_score(manual_scores, judge_scores_llama)\n",
    "\n",
    "print(f\"Cohen‚Äôs Kappa (Minerva): {kappa_minerva:.2f}\")\n",
    "print(f\"Cohen‚Äôs Kappa (LLaMA): {kappa_llama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e63bc",
   "metadata": {},
   "source": [
    "# üíæ Salvataggio risultati JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9004b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(\"groupX-hw2_transl-minerva350M\", archaic_sentences, translations_minerva)\n",
    "save_jsonl(\"groupX-hw2_transl-llama2_7b\", archaic_sentences, translations_llama)\n",
    "save_judging(\"groupX-hw2_transl-judge_minerva\", archaic_sentences, judge_scores_minerva)\n",
    "save_judging(\"groupX-hw2_transl-judge_llama\", archaic_sentences, judge_scores_llama)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d59394",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# Prometheus LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b5f46",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfcb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 21  # Number of samples to process\n",
    "# === 1. Load data ===\n",
    "df_base = pd.read_csv(\"inputs/dataset_human_eval.csv\").head(num_samples) \n",
    "df_mistral = pd.read_csv(\"outputs/dataset_with_mistral_translations.csv\").head(num_samples)\n",
    "df_mt5 = pd.read_csv(\"outputs/dataset_with_mT5_translations.csv\").head(num_samples)\n",
    "df_tinyllama = pd.read_csv(\"outputs/dataset_with_tinyllama_translations.csv\").head(num_samples)\n",
    "df_nllb = pd.read_csv(\"outputs/dataset_with_NLLB_translations.csv\").head(num_samples)\n",
    "df_base[\"mistral\"] = df_mistral[\"generated_translation\"]\n",
    "df_base[\"mistralHS\"] = df_mistral[\"score_human\"]\n",
    "\n",
    "df_base[\"mt5\"] = df_mt5[\"generated_translation\"]\n",
    "df_base[\"mt5HS\"] = df_mt5[\"score_human\"]\n",
    "\n",
    "df_base[\"tinyllama\"] = df_tinyllama[\"generated_translation\"]\n",
    "df_base[\"tinyllamaHS\"] = df_tinyllama[\"score_human\"]\n",
    "\n",
    "df_base[\"nllb\"] = df_nllb[\"generated_translation\"]\n",
    "df_base[\"nllbHS\"] = df_nllb[\"score_human\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be00cb",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a23d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.32s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 2. Load Prometheus Model ===\n",
    "model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "#model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"offload_prometheus\",\n",
    "    offload_buffers=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671b550",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c243d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3. Rubric & prompt ===\n",
    "\n",
    "rubric_data = {\n",
    "    \"criteria\": \"Semantic fidelity of the translation\",\n",
    "    \"score1\": \"Completely wrong, the meaning is unrecognizable.\",\n",
    "    \"score2\": \"Severe meaning errors or omissions or explanations.\",\n",
    "    \"score3\": \"Some inaccuracies, but the general meaning is conveyed.\",\n",
    "    \"score4\": \"Good fidelity, minor non-substantial differences.\",\n",
    "    \"score5\": \"Perfectly faithful to the original meaning.\"\n",
    "}\n",
    "\n",
    "def build_judge_prompt(original, human_translation, model_translation):\n",
    "    return f\"\"\"\n",
    "You are a translation evaluator. Your task is to assign a score from 1 to 5 that reflects how well the model's translation preserves the original meaning.\n",
    "\n",
    "Original:\n",
    "{original}\n",
    "\n",
    "Human:\n",
    "{human_translation}\n",
    "\n",
    "Model:\n",
    "{model_translation}\n",
    "\n",
    "### Score Rubrics:\n",
    "{rubric_data[\"criteria\"]}\n",
    "Score 1: {rubric_data[\"score1\"]}\n",
    "Score 2: {rubric_data[\"score2\"]}\n",
    "Score 3: {rubric_data[\"score3\"]}\n",
    "Score 4: {rubric_data[\"score4\"]}\n",
    "Score 5: {rubric_data[\"score5\"]}\n",
    "\n",
    "Assess the semantic fidelity objectively. Assign the appropriate score based on the rubric.\n",
    "\n",
    "Score (1–5):\n",
    "\"\"\".strip()\n",
    "\n",
    "def extract_score(output):\n",
    "    match = re.search(r\"Score\\s*\\(1[\\-–]5\\):\\s*(\\d)\", output)\n",
    "    return int(match.group(1)) if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5868435",
   "metadata": {},
   "source": [
    "## Run Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▌         | 1/20 [00:05<01:53,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 2/20 [00:11<01:42,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▌        | 3/20 [00:16<01:34,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 4/20 [00:22<01:27,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 5/20 [00:27<01:21,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 6/20 [00:32<01:14,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 7/20 [00:38<01:09,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 8/20 [00:43<01:04,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 9/20 [00:49<00:59,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 10/20 [00:54<00:53,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▌    | 11/20 [00:59<00:48,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 12/20 [01:04<00:42,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▌   | 13/20 [01:10<00:37,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 14/20 [01:15<00:32,  5.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 15/20 [01:20<00:26,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 16/20 [01:26<00:21,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 17/20 [01:31<00:16,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 18/20 [01:36<00:10,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 19/20 [01:42<00:05,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 20/20 [01:47<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved in outputs/prometheus_eval.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Sentencemistral'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     51\u001b[39m df_base.to_csv(\u001b[33m\"\u001b[39m\u001b[33moutputs/prometheus_eval.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, quoting=csv.QUOTE_MINIMAL)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFile saved in outputs/prometheus_eval.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m df_mistral = \u001b[43mdf_base\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistral\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mmistral\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mMistral_translation\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     56\u001b[39m df_nllb    = df_base[[\u001b[33m\"\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnllb\u001b[39m\u001b[33m\"\u001b[39m]].rename(columns={\u001b[33m\"\u001b[39m\u001b[33mnnlb\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNllb_translation\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     57\u001b[39m df_tiny    = df_base[[\u001b[33m\"\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtinyllama\u001b[39m\u001b[33m\"\u001b[39m]].rename(columns={\u001b[33m\"\u001b[39m\u001b[33mtinyllama\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTinyllama_translation\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['Sentencemistral'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# === 4. Score ===\n",
    "score_as = []\n",
    "score_bs = []\n",
    "score_cs = []\n",
    "score_ds = []\n",
    "winners = []\n",
    "\n",
    "for _, row in tqdm(df_base.iterrows(), total=len(df_base)):\n",
    "    for label, translation, score_list in [(\"A\", row[\"mistral\"], score_as), (\"B\", row[\"nllb\"], score_bs),(\"C\", row[\"tinyllama\"], score_cs),(\"D\", row[\"mt5\"], score_ds)]:\n",
    "        prompt = build_judge_prompt(row[\"Sentence\"], row[\"HumanEval\"], translation)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        score = extract_score(output_text)\n",
    "        score_list.append(score)\n",
    "\n",
    "        # Clear memory\n",
    "        del inputs, output_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    score_a = score_as[-1]\n",
    "    score_b = score_bs[-1]\n",
    "    score_c = score_cs[-1]\n",
    "    score_d = score_ds[-1]\n",
    "    if score_a is None or score_b is None or score_c is None or score_d is None:\n",
    "        winners.append(\"?\")\n",
    "    max_score = max(score_a, score_b, score_c, score_d)\n",
    "    temp_winners = []\n",
    "    if score_a==max_score:\n",
    "        temp_winners.append(\"Mistral\")\n",
    "    if score_b==max_score:\n",
    "        temp_winners.append(\"Nllb\") \n",
    "    if score_c==max_score:\n",
    "        temp_winners.append(\"TinyLlama\")\n",
    "    if score_d==max_score:\n",
    "        temp_winners.append(\"Mt5\")\n",
    "    if len(temp_winners) == 4:\n",
    "        winners.append(\"=\") # If all scores are equal\n",
    "    else:\n",
    "        winners.append(\" & \".join(temp_winners))\n",
    "\n",
    "# === 5. Save ===\n",
    "df_base[\"score_a\"] = score_as\n",
    "df_base[\"score_b\"] = score_bs\n",
    "df_base[\"score_c\"] = score_cs\n",
    "df_base[\"score_d\"] = score_ds\n",
    "df_base[\"winner\"] = winners\n",
    "\n",
    "df_base.to_csv(\"outputs/prometheus_eval.csv\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "print(\"File saved in outputs/prometheus_eval.csv\")\n",
    "\n",
    "\n",
    "df_mistral = df_base[[\"Sentence\", \"mistral\"]].rename(columns={\"mistral\": \"Mistral_translation\"})\n",
    "df_nllb    = df_base[[\"Sentence\", \"nllb\"]].rename(columns={\"nllb\": \"Nllb_translation\"})\n",
    "df_tiny    = df_base[[\"Sentence\",\"tinyllama\"]].rename(columns={\"tinyllama\": \"Tinyllama_translation\"})\n",
    "df_mt5     = df_base[[\"Sentence\",\"mt5\"]].rename(columns={\"mt5\": \"Mt5_translation\"})\n",
    "\n",
    "df_prometheus = df_base[[\"Sentence\",\"score_a\", \"score_b\", \"score_c\", \"score_d\", \"winner\"]].rename(columns={\"score_a\": \"mistral_score\", \"score_b\": \"nllb_score\", \"score_c\": \"tinyllama_score\", \"score_d\": \"mt5_score\"})\n",
    "\n",
    "# Save the DataFrames as JSONL files\n",
    "df_mistral.to_json(\"outputs/salmonators-hw2_transl-mistral.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_nllb.to_json(\"outputs/salmonators-hw2_transl-nllb.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_tiny.to_json(\"outputs/salmonators-hw2_transl-tinyllama.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_mt5.to_json(\"outputs/salmonators-hw2_transl-mt5.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_prometheus.to_json(\"outputs/salmonators-hw2_transl-judge.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Files saved in outputs/:\")\n",
    "print(\"- prometheus_eval.csv (completo)\")\n",
    "print(\"- outputs/salmonators-hw2_transl-mistral.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-nllb.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-tinyllama.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-mt5.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b571c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concordance(y_true, y_pred):\n",
    "    print(\"== Unique values ==\")\n",
    "    print(\"Human:\", sorted(set(y_true)))\n",
    "    print(\"Model:\", sorted(set(y_pred)))\n",
    "    \n",
    "    if len(set(y_true)) < 2 or len(set(y_pred)) < 2:\n",
    "        print(\"Not enough variability to be able to calculate Cohen's Kappa.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n== Cohen’s Kappa ==\")\n",
    "    print(f\"{cohen_kappa_score(y_true, y_pred):.3f}\")\n",
    "    return cohen_kappa_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb54fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MISTRAL\n",
      "== Unique values ==\n",
      "Human: [3, 4, 5]\n",
      "Model: [3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.565\n",
      "\n",
      " NLLB\n",
      "== Unique values ==\n",
      "Human: [2, 3, 4, 5]\n",
      "Model: [1, 3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.508\n",
      "\n",
      " TinyLlama\n",
      "== Unique values ==\n",
      "Human: [1, 2, 3, 4, 5]\n",
      "Model: [1, 2, 3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.219\n",
      "\n",
      " Mt5\n",
      "== Unique values ==\n",
      "Human: [2, 3, 4, 5]\n",
      "Model: [1, 3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.128\n"
     ]
    }
   ],
   "source": [
    "df_base = pd.read_csv(\"outputs/prometheus_eval.csv\")\n",
    "print(\"\\n MISTRAL\")\n",
    "ck_a=evaluate_concordance(df_base[\"mistralHS\"], df_base[\"score_a\"])\n",
    "print(\"\\n NLLB\")\n",
    "ck_b=evaluate_concordance(df_base[\"nllbHS\"], df_base[\"score_b\"])\n",
    "print(\"\\n TinyLlama\")\n",
    "ck_c=evaluate_concordance(df_base[\"tinyllamaHS\"], df_base[\"score_c\"])\n",
    "print(\"\\n Mt5\")\n",
    "ck_d=evaluate_concordance(df_base[\"mt5HS\"], df_base[\"score_d\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b855dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of cohen kappa scores\n",
    "scores = [ck_a, ck_b, ck_c, ck_d]\n",
    "labels = ['Mistral', 'NLLB', 'TinyLlama', 'Mt5']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Cohen's Kappa Scores for Translation Models\")\n",
    "plt.xlabel(\"Translation Model\")\n",
    "plt.ylabel(\"Cohen's Kappa Score\")\n",
    "bars=plt.bar(labels, scores, color=['blue', 'orange', 'green', 'red'])\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f'{yval:.3f}', ha='center', va='bottom')\n",
    "plt.ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

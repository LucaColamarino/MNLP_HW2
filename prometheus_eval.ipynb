{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca246820",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colam\\Documents\\GitHub\\MNLP_HW2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# üîç Prometheus LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b5f46",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 15  # Numero di campioni da processare\n",
    "# === 1. Carica i dati ===\n",
    "df_base = pd.read_csv(\"inputs/dataset_human_eval.csv\").head(num_samples)  # usa .head(N) per limitare\n",
    "df_mistral = pd.read_csv(\"outputs/dataset_with_mistral_translations.csv\").head(num_samples)\n",
    "df_mt5 = pd.read_csv(\"outputs/dataset_with_mT5_translations.csv\").head(num_samples)\n",
    "df_tinyllama = pd.read_csv(\"outputs/dataset_with_tinyllama_translations.csv\").head(num_samples)\n",
    "df_nllb = pd.read_csv(\"outputs/dataset_with_NLLB_translations.csv\").head(num_samples)\n",
    "df_base[\"mistral\"] = df_mistral[\"generated_translation\"]\n",
    "df_base[\"mistralHS\"] = df_mistral[\"score_human\"]\n",
    "\n",
    "df_base[\"mt5\"] = df_mt5[\"generated_translation\"]\n",
    "df_base[\"mt5HS\"] = df_mt5[\"score_human\"]\n",
    "\n",
    "df_base[\"tinyllama\"] = df_tinyllama[\"generated_translation\"]\n",
    "df_base[\"tinyllamaHS\"] = df_tinyllama[\"score_human\"]\n",
    "\n",
    "df_base[\"nllb\"] = df_nllb[\"generated_translation\"]\n",
    "df_base[\"nllbHS\"] = df_nllb[\"score_human\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be00cb",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a23d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:09<00:00,  1.22s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 2. Carica il modello Prometheus ===\n",
    "model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "#model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"offload_prometheus\",\n",
    "    offload_buffers=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671b550",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c243d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3. Rubrica e prompt ===\n",
    "\n",
    "rubric_data = {\n",
    "    \"criteria\": \"Semantic fidelity of the translation\",\n",
    "    \"score1\": \"Completely wrong, the meaning is unrecognizable.\",\n",
    "    \"score2\": \"Severe meaning errors or omissions or explanations.\",\n",
    "    \"score3\": \"Some inaccuracies, but the general meaning is conveyed.\",\n",
    "    \"score4\": \"Good fidelity, minor non-substantial differences.\",\n",
    "    \"score5\": \"Perfectly faithful to the original meaning.\"\n",
    "}\n",
    "\n",
    "def build_judge_prompt(original, human_translation, model_translation):\n",
    "    return f\"\"\"\n",
    "You are a translation evaluator. Your task is to assign a score from 1 to 5 that reflects how well the model's translation preserves the original meaning.\n",
    "\n",
    "Original:\n",
    "{original}\n",
    "\n",
    "Human:\n",
    "{human_translation}\n",
    "\n",
    "Model:\n",
    "{model_translation}\n",
    "\n",
    "### Score Rubrics:\n",
    "{rubric_data[\"criteria\"]}\n",
    "Score 1: {rubric_data[\"score1\"]}\n",
    "Score 2: {rubric_data[\"score2\"]}\n",
    "Score 3: {rubric_data[\"score3\"]}\n",
    "Score 4: {rubric_data[\"score4\"]}\n",
    "Score 5: {rubric_data[\"score5\"]}\n",
    "\n",
    "Assess the semantic fidelity objectively. Assign the appropriate score based on the rubric.\n",
    "\n",
    "Score (1‚Äì5):\n",
    "\"\"\".strip()\n",
    "\n",
    "def estrai_punteggio(output):\n",
    "    match = re.search(r\"Score\\s*\\(1[\\-‚Äì]5\\):\\s*(\\d)\", output)\n",
    "    return int(match.group(1)) if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5868435",
   "metadata": {},
   "source": [
    "## Run Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|‚ñã         | 1/15 [00:05<01:20,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 13%|‚ñà‚ñé        | 2/15 [00:10<01:10,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|‚ñà‚ñà        | 3/15 [00:16<01:04,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|‚ñà‚ñà‚ñã       | 4/15 [00:21<00:57,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 5/15 [00:26<00:52,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 6/15 [00:31<00:47,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 7/15 [00:36<00:41,  5.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8/15 [00:42<00:36,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 9/15 [00:47<00:31,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 10/15 [00:52<00:26,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 11/15 [00:58<00:21,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 12/15 [01:03<00:15,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 13/15 [01:08<00:10,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 14/15 [01:13<00:05,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:19<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File salvato in outputs/prometheus_eval.csv\n",
      "‚úÖ File salvati in outputs/:\n",
      "- prometheus_eval.csv (completo)\n",
      "- outputs/salmonators-hw2_transl-judge_mistral.jsonl.csv\n",
      "- salmonators-hw2_transl-judge_nnlb.jsonl\n",
      "- salmonators-hw2_transl-judge_tinyllama.jsonl\n",
      "- salmonators-hw2_transl-judge_mt5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 4. Valutazione ===\n",
    "score_as = []\n",
    "score_bs = []\n",
    "score_cs = []\n",
    "score_ds = []\n",
    "winners = []\n",
    "\n",
    "for _, row in tqdm(df_base.iterrows(), total=len(df_base)):\n",
    "    for label, translation, score_list in [(\"A\", row[\"mistral\"], score_as), (\"B\", row[\"nllb\"], score_bs),(\"C\", row[\"tinyllama\"], score_cs),(\"D\", row[\"mt5\"], score_ds)]:\n",
    "        prompt = build_judge_prompt(row[\"Sentence\"], row[\"HumanEval\"], translation)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        #print(output_text)\n",
    "        score = estrai_punteggio(output_text)\n",
    "        #print(score)\n",
    "        score_list.append(score)\n",
    "\n",
    "        # Pulisce memoria\n",
    "        del inputs, output_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    score_a = score_as[-1]\n",
    "    score_b = score_bs[-1]\n",
    "    score_c = score_cs[-1]\n",
    "    score_d = score_ds[-1]\n",
    "    if score_a is None or score_b is None:\n",
    "        winners.append(\"?\")\n",
    "    elif score_a > score_b and score_a > score_c and score_a > score_d:\n",
    "        winners.append(\"Mistral\")\n",
    "    elif score_b > score_a and score_b > score_c and score_b > score_d:\n",
    "        winners.append(\"Nllb\")\n",
    "    elif score_c > score_a and score_c > score_b and score_c > score_d:\n",
    "        winners.append(\"TinyLlama\")\n",
    "    elif score_d > score_a and score_d > score_b and score_d > score_c:\n",
    "        winners.append(\"Mt5\") \n",
    "    else:\n",
    "        winners.append(\"=\")\n",
    "\n",
    "# === 5. Salvataggio ===\n",
    "df_base[\"score_a\"] = score_as\n",
    "df_base[\"score_b\"] = score_bs\n",
    "df_base[\"score_c\"] = score_cs\n",
    "df_base[\"score_d\"] = score_ds\n",
    "df_base[\"winner\"] = winners\n",
    "\n",
    "df_base.to_csv(\"outputs/prometheus_eval.csv\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "print(\"‚úÖ File salvato in outputs/prometheus_eval.csv\")\n",
    "\n",
    "\n",
    "# Crea file separati per ciascun modello\n",
    "df_mistral = df_base[[\"Sentence\", \"HumanEval\", \"mistral\", \"score_a\", \"winner\"]].rename(columns={\"mistral\": \"translation\", \"score_a\": \"score\"})\n",
    "df_nllb    = df_base[[\"Sentence\", \"HumanEval\", \"nllb\", \"score_b\", \"winner\"]].rename(columns={\"nnlb\": \"translation\", \"score_b\": \"score\"})\n",
    "df_tiny    = df_base[[\"Sentence\", \"HumanEval\", \"tinyllama\", \"score_c\", \"winner\"]].rename(columns={\"tinyllama\": \"translation\", \"score_c\": \"score\"})\n",
    "df_mt5     = df_base[[\"Sentence\", \"HumanEval\", \"mt5\", \"score_d\", \"winner\"]].rename(columns={\"mt5\": \"translation\", \"score_d\": \"score\"})\n",
    "\n",
    "# Salvataggio file\n",
    "df_mistral.to_json(\"outputs/salmonators-hw2_transl-judge_mistral.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_nllb.to_json(\"outputs/salmonators-hw2_transl-judge_nllb.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_tiny.to_json(\"outputs/salmonators-hw2_transl-judge_tinyllama.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_mt5.to_json(\"outputs/salmonators-hw2_transl-judge_mt5.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"‚úÖ File salvati in outputs/:\")\n",
    "print(\"- prometheus_eval.csv (completo)\")\n",
    "print(\"- outputs/salmonators-hw2_transl-judge_mistral.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_nllb.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_tinyllama.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_mt5.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# üìä Calcolo concordanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b571c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concordance(y_true, y_pred):\n",
    "    print(\"== Valori unici ==\")\n",
    "    print(\"Umano:\", sorted(set(y_true)))\n",
    "    print(\"Modello:\", sorted(set(y_pred)))\n",
    "    \n",
    "    if len(set(y_true)) < 2 or len(set(y_pred)) < 2:\n",
    "        print(\"‚ö†Ô∏è Non abbastanza variabilit√† per calcolare Cohen's Kappa.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n== Cohen‚Äôs Kappa ==\")\n",
    "    print(f\"{cohen_kappa_score(y_true, y_pred):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL\n",
      "== Valori unici ==\n",
      "Umano: [3, 4, 5]\n",
      "Modello: [3, 5]\n",
      "\n",
      "== Cohen‚Äôs Kappa ==\n",
      "0.574\n",
      "NNLB\n",
      "== Valori unici ==\n",
      "Umano: [2, 3, 4, 5]\n",
      "Modello: [1, 3, 5]\n",
      "\n",
      "== Cohen‚Äôs Kappa ==\n",
      "0.434\n",
      "TinyLlama\n",
      "== Valori unici ==\n",
      "Umano: [1, 3, 4, 5]\n",
      "Modello: [1, 3, 5]\n",
      "\n",
      "== Cohen‚Äôs Kappa ==\n",
      "0.029\n",
      "Mt5\n",
      "== Valori unici ==\n",
      "Umano: [2, 3, 4, 5]\n",
      "Modello: [1, 5]\n",
      "\n",
      "== Cohen‚Äôs Kappa ==\n",
      "0.062\n"
     ]
    }
   ],
   "source": [
    "df_base = pd.read_csv(\"outputs/prometheus_eval.csv\")\n",
    "print(\"MISTRAL\")\n",
    "evaluate_concordance(df_base[\"mistralHS\"], df_base[\"score_a\"])\n",
    "print(\"NLLB\")\n",
    "evaluate_concordance(df_base[\"nllbHS\"], df_base[\"score_b\"])\n",
    "print(\"TinyLlama\")\n",
    "evaluate_concordance(df_base[\"tinyllamaHS\"], df_base[\"score_c\"])\n",
    "print(\"Mt5\")\n",
    "evaluate_concordance(df_base[\"mt5HS\"], df_base[\"score_d\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

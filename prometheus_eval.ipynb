{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d59394",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b019484",
   "metadata": {},
   "source": [
    "# Prometheus LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b5f46",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bfcb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 21  # Number of samples to process\n",
    "# === 1. Load data ===\n",
    "df_base = pd.read_csv(\"inputs/dataset_human_eval.csv\").head(num_samples) \n",
    "df_mistral = pd.read_csv(\"outputs/dataset_with_mistral_translations.csv\").head(num_samples)\n",
    "df_mt5 = pd.read_csv(\"outputs/dataset_with_mT5_translations.csv\").head(num_samples)\n",
    "df_tinyllama = pd.read_csv(\"outputs/dataset_with_tinyllama_translations.csv\").head(num_samples)\n",
    "df_nllb = pd.read_csv(\"outputs/dataset_with_NLLB_translations.csv\").head(num_samples)\n",
    "df_base[\"mistral\"] = df_mistral[\"generated_translation\"]\n",
    "df_base[\"mistralHS\"] = df_mistral[\"score_human\"]\n",
    "\n",
    "df_base[\"mt5\"] = df_mt5[\"generated_translation\"]\n",
    "df_base[\"mt5HS\"] = df_mt5[\"score_human\"]\n",
    "\n",
    "df_base[\"tinyllama\"] = df_tinyllama[\"generated_translation\"]\n",
    "df_base[\"tinyllamaHS\"] = df_tinyllama[\"score_human\"]\n",
    "\n",
    "df_base[\"nllb\"] = df_nllb[\"generated_translation\"]\n",
    "df_base[\"nllbHS\"] = df_nllb[\"score_human\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be00cb",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a23d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.56it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 2. Load Prometheus Model ===\n",
    "model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "#model_name = \"Unbabel/M-Prometheus-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    offload_folder=\"offload_prometheus\",\n",
    "    offload_buffers=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671b550",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c243d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3. Rubric & prompt ===\n",
    "\n",
    "rubric_data = {\n",
    "    \"criteria\": \"Semantic fidelity of the translation\",\n",
    "    \"score1\": \"Completely wrong, the meaning is unrecognizable.\",\n",
    "    \"score2\": \"Severe meaning errors or omissions or explanations.\",\n",
    "    \"score3\": \"Some inaccuracies, but the general meaning is conveyed.\",\n",
    "    \"score4\": \"Good fidelity, minor non-substantial differences.\",\n",
    "    \"score5\": \"Perfectly faithful to the original meaning.\"\n",
    "}\n",
    "\n",
    "def build_judge_prompt(original, human_translation, model_translation):\n",
    "    return f\"\"\"\n",
    "You are a translation evaluator. Your task is to assign a score from 1 to 5 that reflects how well the model's translation preserves the original meaning.\n",
    "\n",
    "Original:\n",
    "{original}\n",
    "\n",
    "Human:\n",
    "{human_translation}\n",
    "\n",
    "Model:\n",
    "{model_translation}\n",
    "\n",
    "### Score Rubrics:\n",
    "{rubric_data[\"criteria\"]}\n",
    "Score 1: {rubric_data[\"score1\"]}\n",
    "Score 2: {rubric_data[\"score2\"]}\n",
    "Score 3: {rubric_data[\"score3\"]}\n",
    "Score 4: {rubric_data[\"score4\"]}\n",
    "Score 5: {rubric_data[\"score5\"]}\n",
    "\n",
    "Assess the semantic fidelity objectively. Assign the appropriate score based on the rubric.\n",
    "\n",
    "Score (1–5):\n",
    "\"\"\".strip()\n",
    "\n",
    "def extract_score(output):\n",
    "    match = re.search(r\"Score\\s*\\(1[\\-–]5\\):\\s*(\\d)\", output)\n",
    "    return int(match.group(1)) if match else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5868435",
   "metadata": {},
   "source": [
    "## Run Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|▌         | 1/20 [00:07<02:30,  7.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 2/20 [00:13<01:54,  6.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▌        | 3/20 [00:18<01:38,  5.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|██        | 4/20 [00:23<01:28,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 25%|██▌       | 5/20 [00:28<01:20,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|███       | 6/20 [00:33<01:14,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 7/20 [00:38<01:07,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 40%|████      | 8/20 [00:43<01:01,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|████▌     | 9/20 [00:48<00:56,  5.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 10/20 [00:53<00:50,  5.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|█████▌    | 11/20 [00:58<00:45,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 60%|██████    | 12/20 [01:03<00:39,  4.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▌   | 13/20 [01:08<00:34,  4.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|███████   | 14/20 [01:13<00:29,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 75%|███████▌  | 15/20 [01:17<00:24,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|████████  | 16/20 [01:22<00:19,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 17/20 [01:27<00:14,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 90%|█████████ | 18/20 [01:32<00:09,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|█████████▌| 19/20 [01:37<00:04,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 20/20 [01:42<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File salvato in outputs/prometheus_eval.csv\n",
      "✅ File salvati in outputs/:\n",
      "- prometheus_eval.csv (completo)\n",
      "- outputs/salmonators-hw2_transl-judge_mistral.jsonl\n",
      "- salmonators-hw2_transl-judge_nllb.jsonl\n",
      "- salmonators-hw2_transl-judge_tinyllama.jsonl\n",
      "- salmonators-hw2_transl-judge_mt5.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 4. Score ===\n",
    "score_as = []\n",
    "score_bs = []\n",
    "score_cs = []\n",
    "score_ds = []\n",
    "winners = []\n",
    "\n",
    "for _, row in tqdm(df_base.iterrows(), total=len(df_base)):\n",
    "    for label, translation, score_list in [(\"A\", row[\"mistral\"], score_as), (\"B\", row[\"nllb\"], score_bs),(\"C\", row[\"tinyllama\"], score_cs),(\"D\", row[\"mt5\"], score_ds)]:\n",
    "        prompt = build_judge_prompt(row[\"Sentence\"], row[\"HumanEval\"], translation)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        score = extract_score(output_text)\n",
    "        score_list.append(score)\n",
    "\n",
    "        # Clear memory\n",
    "        del inputs, output_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    score_a = score_as[-1]\n",
    "    score_b = score_bs[-1]\n",
    "    score_c = score_cs[-1]\n",
    "    score_d = score_ds[-1]\n",
    "    if score_a is None or score_b is None:\n",
    "        winners.append(\"?\")\n",
    "    elif score_a > score_b and score_a > score_c and score_a > score_d:\n",
    "        winners.append(\"Mistral\")\n",
    "    elif score_b > score_a and score_b > score_c and score_b > score_d:\n",
    "        winners.append(\"Nllb\")\n",
    "    elif score_c > score_a and score_c > score_b and score_c > score_d:\n",
    "        winners.append(\"TinyLlama\")\n",
    "    elif score_d > score_a and score_d > score_b and score_d > score_c:\n",
    "        winners.append(\"Mt5\") \n",
    "    else:\n",
    "        winners.append(\"=\")\n",
    "\n",
    "# === 5. Save ===\n",
    "df_base[\"score_a\"] = score_as\n",
    "df_base[\"score_b\"] = score_bs\n",
    "df_base[\"score_c\"] = score_cs\n",
    "df_base[\"score_d\"] = score_ds\n",
    "df_base[\"winner\"] = winners\n",
    "\n",
    "df_base.to_csv(\"outputs/prometheus_eval.csv\", index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "print(\"File saved in outputs/prometheus_eval.csv\")\n",
    "\n",
    "\n",
    "df_mistral = df_base[[\"Sentence\", \"HumanEval\", \"mistral\", \"score_a\", \"winner\"]].rename(columns={\"mistral\": \"translation\", \"score_a\": \"score\"})\n",
    "df_nllb    = df_base[[\"Sentence\", \"HumanEval\", \"nllb\", \"score_b\", \"winner\"]].rename(columns={\"nnlb\": \"translation\", \"score_b\": \"score\"})\n",
    "df_tiny    = df_base[[\"Sentence\", \"HumanEval\", \"tinyllama\", \"score_c\", \"winner\"]].rename(columns={\"tinyllama\": \"translation\", \"score_c\": \"score\"})\n",
    "df_mt5     = df_base[[\"Sentence\", \"HumanEval\", \"mt5\", \"score_d\", \"winner\"]].rename(columns={\"mt5\": \"translation\", \"score_d\": \"score\"})\n",
    "\n",
    "# Save the DataFrames as JSONL files\n",
    "df_mistral.to_json(\"outputs/salmonators-hw2_transl-judge_mistral.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_nllb.to_json(\"outputs/salmonators-hw2_transl-judge_nllb.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_tiny.to_json(\"outputs/salmonators-hw2_transl-judge_tinyllama.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_mt5.to_json(\"outputs/salmonators-hw2_transl-judge_mt5.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Files saved in outputs/:\")\n",
    "print(\"- prometheus_eval.csv (completo)\")\n",
    "print(\"- outputs/salmonators-hw2_transl-judge_mistral.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_nllb.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_tinyllama.jsonl\")\n",
    "print(\"- salmonators-hw2_transl-judge_mt5.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126de2e",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b571c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_concordance(y_true, y_pred):\n",
    "    print(\"== Unique values ==\")\n",
    "    print(\"Human:\", sorted(set(y_true)))\n",
    "    print(\"Model:\", sorted(set(y_pred)))\n",
    "    \n",
    "    if len(set(y_true)) < 2 or len(set(y_pred)) < 2:\n",
    "        print(\"Not enough variability to be able to calculate Cohen's Kappa.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n== Cohen’s Kappa ==\")\n",
    "    print(f\"{cohen_kappa_score(y_true, y_pred):.3f}\")\n",
    "    return cohen_kappa_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb54fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MISTRAL\n",
      "== Unique values ==\n",
      "Human: [3, 4, 5]\n",
      "Model: [3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.565\n",
      "\n",
      " NLLB\n",
      "== Unique values ==\n",
      "Human: [2, 3, 4, 5]\n",
      "Model: [1, 3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.508\n",
      "\n",
      " TinyLlama\n",
      "== Unique values ==\n",
      "Human: [2, 3, 4, 5]\n",
      "Model: [1, 2, 3, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.009\n",
      "\n",
      " Mt5\n",
      "== Unique values ==\n",
      "Human: [2, 3, 4, 5]\n",
      "Model: [1, 5]\n",
      "\n",
      "== Cohen’s Kappa ==\n",
      "0.032\n"
     ]
    }
   ],
   "source": [
    "df_base = pd.read_csv(\"outputs/prometheus_eval.csv\")\n",
    "print(\"\\n MISTRAL\")\n",
    "ck_a=evaluate_concordance(df_base[\"mistralHS\"], df_base[\"score_a\"])\n",
    "print(\"\\n NLLB\")\n",
    "ck_b=evaluate_concordance(df_base[\"nllbHS\"], df_base[\"score_b\"])\n",
    "print(\"\\n TinyLlama\")\n",
    "ck_c=evaluate_concordance(df_base[\"tinyllamaHS\"], df_base[\"score_c\"])\n",
    "print(\"\\n Mt5\")\n",
    "ck_d=evaluate_concordance(df_base[\"mt5HS\"], df_base[\"score_d\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b855dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of cohen kappa scores\n",
    "scores = [ck_a, ck_b, ck_c, ck_d]\n",
    "labels = ['Mistral', 'NLLB', 'TinyLlama', 'Mt5']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, scores, color=['blue', 'orange', 'green', 'red'])\n",
    "plt.ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

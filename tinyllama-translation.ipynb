{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12049142,"sourceType":"datasetVersion","datasetId":7582857},{"sourceId":12049910,"sourceType":"datasetVersion","datasetId":7583419}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ancient to Modern Italian Translation with TinyLLaMA and BLOOMZ\n\nThis notebook compares two approaches for translating ancient Italian into modern Italian:\n\n1. **TinyLLaMA (Fine-tuned locally on parallel examples)**\n2. **BLOOMZ (Zero-shot / Few-shot Inference)**\n\n---\n\n## ğŸ”§ Setup","metadata":{}},{"cell_type":"code","source":"# --- 1. Install required libraries ---\n!pip install -q transformers datasets peft bitsandbytes accelerate evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T15:33:11.409080Z","iopub.execute_input":"2025-06-04T15:33:11.409363Z","iopub.status.idle":"2025-06-04T15:34:49.620090Z","shell.execute_reply.started":"2025-06-04T15:33:11.409340Z","shell.execute_reply":"2025-06-04T15:34:49.619366Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- 2. Import modules ---\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n    TrainingArguments, Trainer,\n    DataCollatorForLanguageModeling, BitsAndBytesConfig\n)\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom sklearn.model_selection import train_test_split\nimport evaluate\nimport ast\n\n# --- 3. Load and prepare dataset ---\ndf = pd.read_csv('/kaggle/input/datasets-both/dataset_concatenato.csv')[['text', 'translation']].dropna()\ndf = df.rename(columns={'text': 'ancient', 'translation': 'modern'})\ndf = df.head(200)  # for quick training/testing\n\ndef fix_list_string_to_sentence(text):\n    # Try to parse string list representation to python list\n    try:\n        tokens = ast.literal_eval(text)\n        if isinstance(tokens, list):\n            return \" \".join(tokens)\n    except:\n        pass\n    return text  # fallback if parsing fails\n\ndf['modern'] = df['modern'].apply(fix_list_string_to_sentence)\n\ntrain_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\n\n# --- 4. Load tokenizer and model (4-bit quantized TinyLLaMA) ---\nmodel_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 5. Apply LoRA ---\npeft_config = LoraConfig(\n    r = 16,\n    lora_alpha = 64,\n    lora_dropout = 0.1, target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"none\", task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, peft_config)\n\n# --- 6. Preprocess: tokenize and mask input (prompt-only tuning) ---\ndef preprocess_function(examples):\n    prompts = [f\"Translate from ancient to modern Italian:\\nAncient: {a}\\nModern: {m}{tokenizer.eos_token}\" for a, m in zip(examples['ancient'], examples['modern'])]\n    return tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=256)\n\ntrain_ds = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\nval_ds = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n\n# --- 7. Define training ---\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/tinyllama-ft\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,  # effective batch size = 8\n    num_train_epochs=10,\n    logging_steps=50,\n    eval_strategy='steps',\n    eval_steps=50,\n    save_strategy='epoch',\n    learning_rate=2e-4,\n    fp16=True,\n    report_to='none',\n    gradient_checkpointing=True,\n    warmup_steps=10,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/logs\"\n)\n\ndef clean_decoded_text(text):\n        text = text.strip()\n        if text.startswith(\"[\") and text.endswith(\"]\"):\n            text = text[1:-1]  # remove brackets\n        text = text.replace(\"'\", \"\")  # remove quotes\n        return text.strip()\n\nbleu_metric = evaluate.load(\"bleu\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    if isinstance(preds, np.ndarray) and preds.ndim == 3:\n        preds = np.argmax(preds, axis=-1)\n    if isinstance(preds, torch.Tensor):\n        preds = preds.detach().cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.detach().cpu().numpy()\n    \n    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    decoded_preds = [pred.split(\"Modern:\")[-1].strip() for pred in decoded_preds]\n    decoded_labels = [label.split(\"Modern:\")[-1].strip() for label in decoded_labels]\n\n    pred_tokens = [pred.split() for pred in decoded_preds]\n    label_tokens = [label.split() for label in decoded_labels]\n\n    print(\"Sample prediction:\", decoded_preds[0])\n    print(\"Ground truth:\", decoded_labels[0])\n\n    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"bleu\"]}\n\ntrainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n)\n\n# --- 8. Train the model ---\ntrainer.train()\n\n# --- 9. Load test set ---\ntest_df = pd.read_csv('/kaggle/input/dataset-true/dataset_human_eval.csv')[['Sentence', 'HumanEval']].dropna()\ntest_df = test_df.rename(columns={'Sentence': 'ancient', 'HumanEval': 'modern'})\n\n# --- 10. Inference with few-shot prompt ---\nfew_shot_prompt = \"\"\"Translate from ancient to modern Italian:\n\nAncient: O Deo, coâ€™ mi par forte non so se lo sapete, conâ€™ vâ€™amo di bon core;\nModern: Oddio, come mi sembra difficile non so se lo sapete quanto vi amo;\n\nAncient: Apparve luce, che rendÃ© splendore, che passao per li occhi e â€™l cor ferÃ¬o, ondâ€™io ne sono a tal condizÃ¯one;\nModern: Apparve una luce splendente, che passÃ² per gli occhi e colpÃ¬ il cuore, cosicchÃ© io sono in tale condizione;\n\nAncient: Per lo cammino ch'Ã¨ sÃ¬ aspro e forte, vado cercando sol la mia salute;\nModern: Per il cammino che Ã¨ cosÃ¬ duro e difficile, vado cercando soltanto la mia salvezza;\n\nAncient: {input}\nModern:\"\"\"\n\ndef generate_with_tinyllama(text):\n    prompt = few_shot_prompt.format(input=text)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=128,\n        do_sample=False,     # â† deterministic (greedy) decoding\n        num_beams=4,         # â† beam search helps BLEU\n        temperature=0.7,        # moderate randomness\n        top_p=0.9,              # nucleus sampling\n        early_stopping=True,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Strip everything before the final \"Modern:\" to get clean output\n    return decoded.split(\"Modern:\")[-1].strip()\n\n# --- 11. Generate on test set ---\ntest_df['tinyllama_output'] = test_df['ancient'].apply(generate_with_tinyllama)\n\n# --- 12. Save predictions ---\ntest_df[['ancient', 'modern', 'tinyllama_output']].to_csv(\"/kaggle/working/tinyllama_predictions.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:41:35.096014Z","iopub.execute_input":"2025-06-04T17:41:35.096430Z","iopub.status.idle":"2025-06-04T17:51:47.161597Z","shell.execute_reply.started":"2025-06-04T17:41:35.096405Z","shell.execute_reply":"2025-06-04T17:51:47.161011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b40c1d05d19543908ba45f3ff5f8e5ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3d038510d943649be7a94189c00f09"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/3683008165.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 07:25, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>3.248100</td>\n      <td>2.853111</td>\n      <td>0.128961</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.487300</td>\n      <td>2.813025</td>\n      <td>0.128175</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.195500</td>\n      <td>2.849160</td>\n      <td>0.134546</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Sample prediction: Io non soensovo souggare il m, terra, nÃ© non fure se l, non vedederlo sfavolre davutt'eorno, come un ferro cheantentecente;eso uscito dal fuoco;\nGround truth: Io non potei fissare il sole a lungo, ma neppure cosÃ¬ poco da non vederlo sfavillare tutt'intorno, come un ferro incandescente appena uscito dal fuoco;\nSample prediction: Io non soensova souggare il suo, terra, nÃ© non rimure cosÃ¬ tanto, far vedederlo sfavolre davuttaviaoreeorno, come un ferro cheantentecente;are escito dal fuoco;\nGround truth: Io non potei fissare il sole a lungo, ma neppure cosÃ¬ poco da non vederlo sfavillare tutt'intorno, come un ferro incandescente appena uscito dal fuoco;\nSample prediction: Io non soegva souggare il suo, terra, nÃ© solo rimure cosÃ¬,, far vedederlo sfavolre davuttaviaoreunorno, come un ferro cheantentecente cheare escito dal fuoco;\nGround truth: Io non potei fissare il sole a lungo, ma neppure cosÃ¬ poco da non vederlo sfavillare tutt'intorno, come un ferro incandescente appena uscito dal fuoco;\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from IPython.display import display\ndisplay(test_df[['ancient', 'modern', 'tinyllama_output']].head(30))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T17:52:19.053641Z","iopub.execute_input":"2025-06-04T17:52:19.054341Z","iopub.status.idle":"2025-06-04T17:52:19.063773Z","shell.execute_reply.started":"2025-06-04T17:52:19.054316Z","shell.execute_reply":"2025-06-04T17:52:19.063072Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                      ancient  \\\n0        quella guerra ben fatta l' opera perchÃ© etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno   \n1                                                                   crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.   \n2                                                                                                  Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.   \n3                                                 Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrÃ² piÃ¹ i fati.   \n4                                                                 Officio di questa arte pare che sia dicere appostatamente per fare credere, fine Ã¨ far credere per lo dire.   \n5                                                           Ecco e larghi ventipiovoli caggiono delle risolute nebbie; e potresti credere che tutto il cielo cadesse nel mare   \n6   PerÃ² che or chi spererebbe quello che eziandio questi che non vogliono ancora credere in Cristo, giÃ  veggiono con noi, e perchÃ© nol possono negare, stridono colli denti.   \n7                                                                                                I vendimenti de' morti et le presure de' vivi fece la frode d'uno feroce re.   \n8                         AcciocchÃ© quegli, il quale ora per le sue gran reitÃ  Ã¨ feroce e onorevole, egli d'ogni male afflitto e tormentato della impietÃ  verso il mio padre.   \n9                                                                                                        Gli uomini spessamente a stare fermi nella bugia incontra la veritÃ .   \n10                                                                                 Marco Cornelio ch'era de' dieci compagni, studiosamente  si riservÃ² di parlare all'ultimo.   \n11                                                                                                                               cose ch'io sapeva che erano fatte in Italia.   \n12                                                                                                            Corbio nipote d' Ortensio menÃ² sua vita piÃ¹ bassa e piÃ¹ viziosa   \n13                                                            AltressÃ¬ uno amante chiamando merzÃ© alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.   \n14                                                                   Io mi ricordo (ch. 347) che essendo adirato scapigliai la mia donna. Ohi, quanti dÃ¬ questa ira mi tolse!   \n15                                                                                                       colui del quale tu tti solevi dolere ch' era amante della tua donna;   \n16                                                                            Ma no sapeano giÃ  le nomora di coloro dela congiurazione, chÃ© la donna no nominava giÃ  li nomi.   \n17                                    Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\".   \n18                                    A Milano fue ripressa la malvagitÃ  d' una donna in simile bugÃ¬a, nel tempo medesimo di questo signore della republica, in questo modo:    \n\n                                                                                                                                                                    modern  \\\n0   Quella guerra fu condotta bene perchÃ¨ etc. Dallâ€™altra parte, Aiace era un cavaliere franco e valoroso nelle armi, di grande statura, ma non possedeva grande saggezza.   \n1                                                                           Ãˆ crudele, e punisce ogni colpa come vuole la legge, e non perdona alcun cavaliere che sbagli.   \n2                                                                                                   Ponzio Aufidiano, cavaliere romano, non fu dotato di un animo diverso.   \n3                                      Se questo piace a tutti e se il tempo ha bisogno di avere Pompeo come cavaliere e non come compagno, non ostacolerÃ² piÃ¹ il destino.   \n4                            Il compito di questâ€™arte sembra essere quello di parlare in modo adatto per convincere; lo scopo Ã¨ far credere qualcosa attraverso le parole.   \n5                                                             Ecco che forti piogge scendono dalle dense nebbie; potresti pensare che tutto il cielo stia cadendo in mare.   \n6   PerchÃ© ora chi spererebbe ciÃ² che anche quelli che non vogliono ancora credere in Cristo, giÃ  vedono insieme a noi, e poichÃ© non lo possono negare, stringono i denti.   \n7                                                                                 Le vendite dei morti e le pressioni sui vivi furono opera dellâ€™inganno di un re crudele.   \n8                            CosÃ¬ che colui, che ora Ã¨ temuto e onorato per i suoi grandi delitti, sia tormentato da ogni male a causa della sua crudeltÃ  verso mio padre.   \n9                                                                                                 Spesso gli uomini incontrano la veritÃ  mentre stanno fermi in una bugia.   \n10                                                                 Marco Cornelio, che era uno dei dieci compagni, si riservÃ² di parlare per ultimo con grande attenzione.   \n11                                                                                                                              Cose che sapevo essere avvenute in Italia.   \n12                                                                                                         Corbio, nipote di Ortensio, visse una vita piÃ¹ bassa e viziosa.   \n13                                                 CosÃ¬ anche un innamorato, chiedendo pietÃ  alla sua donna, pronuncia molte parole e motivi, e lei risponde difendendosi.   \n14                                                Ricordo che, essendo arrabbiato, scompigliai i capelli della mia donna. AhimÃ¨, quanti giorni persi a causa di quellâ€™ira!   \n15                                                                                                 Colui di cui ti lamentavi spesso perchÃ© era innamorato della tua donna;   \n16                                                                                Ma ancora non conoscevano i nomi dei cospiratori, perchÃ© la donna non li aveva rivelati.   \n17                                                            Credi? Certo, quando partÃ¬, ti disse: â€œO mia fedele donna, ti affido al mio posto il nostro ospite troianoâ€.   \n18             A Milano fu fermata la malvagitÃ  di una donna colpevole di una simile bugia, proprio durante il tempo di quel governatore della repubblica, in questo modo:   \n\n                                                                                                                                                                                                                               tinyllama_output  \n0                                                                                                   Questa guerra Ã¨ stata fatta perchÃ© Etaces era un cavaliere coraggioso e profondo in pensiero, ma non era sufficientemente dotato per l'arma  \n1                                                                                                         cruenta, e di tutte le colpe che ho commesso pigno giudizio, come dice la legge, ed a un cavaliere perdono la pena per i miei pecchi.  \n2                                                                                                                                                                  Non d' alcuna forza d' animo fu adornato Ponzio Aufidiano, romano cavaliere;  \n3                                                                                                              Se questo piace a tutti e se il tempo ha bisogno di avere Pompeo come cavaliere e non come compagno, non riprenderÃ² piÃ¹ i fatti.  \n4                                                                                                      L'arte di questa disciplina sembra essere quella di dare la parola corretta per far credere, c'Ã¨ fine di fare credere per l'espressione.  \n5                                                                                                                              Ecco le ciglia delle nebbie che caggiano i ventopiovoli, e potresti credere che tutto il cielo cadesse nel mare;  \n6                                                                                                  PoichÃ© chi spera di questo che non vogliono ancora credere in Cristo, giÃ  vedono con noi, e perchÃ© non possono negare, stridono colli denti.  \n7                                                                                                                                                                                                                     Il cammino della luce che  \n8                                                                                 Altrimenti quegli uomini che ora per le sue grandi reitte Ã¨ feroce e onorevole, egli d'ogni malessentimento e tormento del suo pessimismo verso il mio padre.  \n9                                                                                                                                                                                   Apparve una luce splendente, che passÃ² per gli occhi e colp  \n10                                                                                                                            Ora mi sembra che non sia soltanto la mia salute, ma anche la mia vita, che si muove in un cammino aspro e forte.  \n11                                                                                                                                                                                             cose che io conoscevo che erano fatte in Italia.  \n12                                                                             Corbio nipote di Ortensio menÃ² la sua vita di piÃ¹ bassa e piÃ¹ viva.\\n\\nAfter the translation: Oddio, come mi sembra difficile non so se lo sapete quanto vi amo;  \n13                                                                                                                            AltressÃ¬ un amante chiama l'altra donna mia, con le parole e con gli argomenti molte, ella difende in suo favore.  \n14                                                                                                                         Io ricordo (ch. 347) che era necessario scapolare la mia moglie. Ohi, tutti i giorni di questa furia mi hanno tolto!  \n15                                                                                                                                                                                      Questo cammino, che Ã¨ cosÃ¬ duro e difficile, mi fa sent  \n16  Ma non sapevano giÃ  i nomi delle creature della congiurazione, poichÃ© la donna non nominava giÃ  i nomi di essi.\\n\\nAncient: Ma non sapevano giÃ  i nomi delle creature della congiurazione, poichÃ© la donna non nominava giÃ  i nomi di essi.  \n17                                                                                                                                                                                                Creti? SÃ¬, quando i loro corpi si muovono, el  \n18                                                                                                     Oddio, come mi sembra difficile non so se lo sapete quanto vi amo;\\n\\nOddio, come mi sembra difficile non so se lo sapete quanto vi amo;  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ancient</th>\n      <th>modern</th>\n      <th>tinyllama_output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>quella guerra ben fatta l' opera perchÃ© etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno</td>\n      <td>Quella guerra fu condotta bene perchÃ¨ etc. Dallâ€™altra parte, Aiace era un cavaliere franco e valoroso nelle armi, di grande statura, ma non possedeva grande saggezza.</td>\n      <td>Questa guerra Ã¨ stata fatta perchÃ© Etaces era un cavaliere coraggioso e profondo in pensiero, ma non era sufficientemente dotato per l'arma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.</td>\n      <td>Ãˆ crudele, e punisce ogni colpa come vuole la legge, e non perdona alcun cavaliere che sbagli.</td>\n      <td>cruenta, e di tutte le colpe che ho commesso pigno giudizio, come dice la legge, ed a un cavaliere perdono la pena per i miei pecchi.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.</td>\n      <td>Ponzio Aufidiano, cavaliere romano, non fu dotato di un animo diverso.</td>\n      <td>Non d' alcuna forza d' animo fu adornato Ponzio Aufidiano, romano cavaliere;</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrÃ² piÃ¹ i fati.</td>\n      <td>Se questo piace a tutti e se il tempo ha bisogno di avere Pompeo come cavaliere e non come compagno, non ostacolerÃ² piÃ¹ il destino.</td>\n      <td>Se questo piace a tutti e se il tempo ha bisogno di avere Pompeo come cavaliere e non come compagno, non riprenderÃ² piÃ¹ i fatti.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Officio di questa arte pare che sia dicere appostatamente per fare credere, fine Ã¨ far credere per lo dire.</td>\n      <td>Il compito di questâ€™arte sembra essere quello di parlare in modo adatto per convincere; lo scopo Ã¨ far credere qualcosa attraverso le parole.</td>\n      <td>L'arte di questa disciplina sembra essere quella di dare la parola corretta per far credere, c'Ã¨ fine di fare credere per l'espressione.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Ecco e larghi ventipiovoli caggiono delle risolute nebbie; e potresti credere che tutto il cielo cadesse nel mare</td>\n      <td>Ecco che forti piogge scendono dalle dense nebbie; potresti pensare che tutto il cielo stia cadendo in mare.</td>\n      <td>Ecco le ciglia delle nebbie che caggiano i ventopiovoli, e potresti credere che tutto il cielo cadesse nel mare;</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>PerÃ² che or chi spererebbe quello che eziandio questi che non vogliono ancora credere in Cristo, giÃ  veggiono con noi, e perchÃ© nol possono negare, stridono colli denti.</td>\n      <td>PerchÃ© ora chi spererebbe ciÃ² che anche quelli che non vogliono ancora credere in Cristo, giÃ  vedono insieme a noi, e poichÃ© non lo possono negare, stringono i denti.</td>\n      <td>PoichÃ© chi spera di questo che non vogliono ancora credere in Cristo, giÃ  vedono con noi, e perchÃ© non possono negare, stridono colli denti.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I vendimenti de' morti et le presure de' vivi fece la frode d'uno feroce re.</td>\n      <td>Le vendite dei morti e le pressioni sui vivi furono opera dellâ€™inganno di un re crudele.</td>\n      <td>Il cammino della luce che</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>AcciocchÃ© quegli, il quale ora per le sue gran reitÃ  Ã¨ feroce e onorevole, egli d'ogni male afflitto e tormentato della impietÃ  verso il mio padre.</td>\n      <td>CosÃ¬ che colui, che ora Ã¨ temuto e onorato per i suoi grandi delitti, sia tormentato da ogni male a causa della sua crudeltÃ  verso mio padre.</td>\n      <td>Altrimenti quegli uomini che ora per le sue grandi reitte Ã¨ feroce e onorevole, egli d'ogni malessentimento e tormento del suo pessimismo verso il mio padre.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Gli uomini spessamente a stare fermi nella bugia incontra la veritÃ .</td>\n      <td>Spesso gli uomini incontrano la veritÃ  mentre stanno fermi in una bugia.</td>\n      <td>Apparve una luce splendente, che passÃ² per gli occhi e colp</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Marco Cornelio ch'era de' dieci compagni, studiosamente  si riservÃ² di parlare all'ultimo.</td>\n      <td>Marco Cornelio, che era uno dei dieci compagni, si riservÃ² di parlare per ultimo con grande attenzione.</td>\n      <td>Ora mi sembra che non sia soltanto la mia salute, ma anche la mia vita, che si muove in un cammino aspro e forte.</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>cose ch'io sapeva che erano fatte in Italia.</td>\n      <td>Cose che sapevo essere avvenute in Italia.</td>\n      <td>cose che io conoscevo che erano fatte in Italia.</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Corbio nipote d' Ortensio menÃ² sua vita piÃ¹ bassa e piÃ¹ viziosa</td>\n      <td>Corbio, nipote di Ortensio, visse una vita piÃ¹ bassa e viziosa.</td>\n      <td>Corbio nipote di Ortensio menÃ² la sua vita di piÃ¹ bassa e piÃ¹ viva.\\n\\nAfter the translation: Oddio, come mi sembra difficile non so se lo sapete quanto vi amo;</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>AltressÃ¬ uno amante chiamando merzÃ© alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.</td>\n      <td>CosÃ¬ anche un innamorato, chiedendo pietÃ  alla sua donna, pronuncia molte parole e motivi, e lei risponde difendendosi.</td>\n      <td>AltressÃ¬ un amante chiama l'altra donna mia, con le parole e con gli argomenti molte, ella difende in suo favore.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Io mi ricordo (ch. 347) che essendo adirato scapigliai la mia donna. Ohi, quanti dÃ¬ questa ira mi tolse!</td>\n      <td>Ricordo che, essendo arrabbiato, scompigliai i capelli della mia donna. AhimÃ¨, quanti giorni persi a causa di quellâ€™ira!</td>\n      <td>Io ricordo (ch. 347) che era necessario scapolare la mia moglie. Ohi, tutti i giorni di questa furia mi hanno tolto!</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>colui del quale tu tti solevi dolere ch' era amante della tua donna;</td>\n      <td>Colui di cui ti lamentavi spesso perchÃ© era innamorato della tua donna;</td>\n      <td>Questo cammino, che Ã¨ cosÃ¬ duro e difficile, mi fa sent</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Ma no sapeano giÃ  le nomora di coloro dela congiurazione, chÃ© la donna no nominava giÃ  li nomi.</td>\n      <td>Ma ancora non conoscevano i nomi dei cospiratori, perchÃ© la donna non li aveva rivelati.</td>\n      <td>Ma non sapevano giÃ  i nomi delle creature della congiurazione, poichÃ© la donna non nominava giÃ  i nomi di essi.\\n\\nAncient: Ma non sapevano giÃ  i nomi delle creature della congiurazione, poichÃ© la donna non nominava giÃ  i nomi di essi.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\".</td>\n      <td>Credi? Certo, quando partÃ¬, ti disse: â€œO mia fedele donna, ti affido al mio posto il nostro ospite troianoâ€.</td>\n      <td>Creti? SÃ¬, quando i loro corpi si muovono, el</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>A Milano fue ripressa la malvagitÃ  d' una donna in simile bugÃ¬a, nel tempo medesimo di questo signore della republica, in questo modo:</td>\n      <td>A Milano fu fermata la malvagitÃ  di una donna colpevole di una simile bugia, proprio durante il tempo di quel governatore della repubblica, in questo modo:</td>\n      <td>Oddio, come mi sembra difficile non so se lo sapete quanto vi amo;\\n\\nOddio, come mi sembra difficile non so se lo sapete quanto vi amo;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"test_dataset = pd.read_csv('/kaggle/input/datasets-both/dataset_cleaned (1).csv')[['Sentence']].dropna()\ntest_dataset = test_dataset.rename(columns={'Sentence': 'ancient'})\n\ntest_dataset['tinyllama_output'] = test_dataset['ancient'].apply(generate_with_tinyllama)\ntest_dataset[['ancient', 'tinyllama_output']].to_csv(\"/kaggle/working/tinyllama_test_predictions.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T18:06:52.740151Z","iopub.execute_input":"2025-06-04T18:06:52.740463Z","iopub.status.idle":"2025-06-04T18:19:53.888981Z","shell.execute_reply.started":"2025-06-04T18:06:52.740442Z","shell.execute_reply":"2025-06-04T18:19:53.888418Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"}],"execution_count":32}]}
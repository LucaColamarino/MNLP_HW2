{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ancient to Modern Italian Translation with TinyLLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T14:34:44.537211Z",
     "iopub.status.busy": "2025-06-06T14:34:44.536901Z",
     "iopub.status.idle": "2025-06-06T14:36:06.486222Z",
     "shell.execute_reply": "2025-06-06T14:36:06.485371Z",
     "shell.execute_reply.started": "2025-06-06T14:34:44.537186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets peft bitsandbytes accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, BitsAndBytesConfig,\n",
    "    StoppingCriteria, StoppingCriteriaList\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/inputs/dataset_concatenato.csv')[['text', 'translation']].dropna()\n",
    "df = df.rename(columns={'text': 'ancient', 'translation': 'modern'})\n",
    "\n",
    "# Convert a string representation of a list to a string sentence\n",
    "def fix_list_string_to_sentence(text): \n",
    "    try:\n",
    "        tokens = ast.literal_eval(text)\n",
    "        if isinstance(tokens, list):\n",
    "            return \" \".join(tokens)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "# Apply the function to both columns\n",
    "df['ancient'] = df['ancient'].apply(fix_list_string_to_sentence)\n",
    "df['modern'] = df['modern'].apply(fix_list_string_to_sentence)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # Model ID for TinyLlama\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # Load the tokenizer for TinyLlama\n",
    "if tokenizer.pad_token is None: # Set the pad token if it is not already set\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Fine-Tuning configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig( \n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Load the model with 4-bit quantization and prepare it for training\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "# Define the target modules for LoRA\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "peft_config = LoraConfig(\n",
    "    r=32, # Rank for LoRA\n",
    "    lora_alpha=64, # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # Task type for the model\n",
    ")\n",
    "\n",
    "# Get the PEFT model with the LoRA configuration\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for the model\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Tokenizes the input data and prepares it for training\n",
    "def preprocess_function(examples): \n",
    "    all_input_ids = []\n",
    "    all_labels = []\n",
    "    # System prompt that guides the model's behavior during translation.\n",
    "    # It sets the context for the model to act as an expert translator.\n",
    "    SYSTEM_PROMPT = \"You are an expert translator from ancient to modern Italian.\" \n",
    "\n",
    "    for ancient, modern in zip(examples['ancient'], examples['modern']):\n",
    "        # Constructs the full text by combining the system prompt, ancient text, and modern translation.\n",
    "        full_text = (\n",
    "            f\"<|system|>\\n{SYSTEM_PROMPT}{tokenizer.eos_token}\\n\"\n",
    "            f\"<|user|>\\n{ancient}{tokenizer.eos_token}\\n\"\n",
    "            f\"<|assistant|>\\n{modern}{tokenizer.eos_token}\"\n",
    "        )\n",
    "        # Constructs the prompt part of the text, which is used to guide the model.\n",
    "        prompt_only = (\n",
    "            f\"<|system|>\\n{SYSTEM_PROMPT}{tokenizer.eos_token}\\n\"\n",
    "            f\"<|user|>\\n{ancient}{tokenizer.eos_token}\\n\"\n",
    "            f\"<|assistant|>\\n\"\n",
    "        )\n",
    "\n",
    "        # Tokenizes the full text and the prompt part separately.\n",
    "        tokenized_full = tokenizer(full_text, max_length=MAX_LENGTH, truncation=True, padding=False)\n",
    "        tokenized_prompt = tokenizer(prompt_only, max_length=MAX_LENGTH, truncation=True, padding=False)\n",
    "        \n",
    "        # Extracts the input_ids from the tokenized full text and calculates the prompt length.\n",
    "        input_ids = tokenized_full['input_ids']\n",
    "        prompt_length = len(tokenized_prompt['input_ids'])\n",
    "        \n",
    "        # Labels are set to -100 for the prompt part to ignore it during loss calculation.\n",
    "        labels = [-100] * prompt_length + input_ids[prompt_length:]\n",
    "        \n",
    "        # Padding the input_ids and labels to ensure they are of equal length.\n",
    "        padding_length = MAX_LENGTH - len(input_ids) \n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "        \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    return {\"input_ids\": all_input_ids, \"labels\": all_labels}\n",
    "\n",
    "train_ds = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
    "val_ds = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments and Metrics\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients over 4 steps to simulate a larger batch size\n",
    "    num_train_epochs=4,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps', # Evaluate every 10 steps\n",
    "    eval_steps=10,\n",
    "    save_strategy='steps', # Save the model every 10 steps\n",
    "    save_steps=10,\n",
    "    learning_rate=1e-5, # Learning rate for training\n",
    "    report_to='none',\n",
    "    gradient_checkpointing=True, # Enable gradient checkpointing to save memory\n",
    "    warmup_ratio=0.1, # Warmup ratio for learning rate scheduler\n",
    "    weight_decay=0.01, # Weight decay for regularization\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2 # Limit the number of saved checkpoints\n",
    ")\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# Define the compute_metrics function for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Convert logits to predictions\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Isolate the assistant's response for both predictions and labels\n",
    "    cleaned_preds = [pred.split(\"<|assistant|>\")[-1].strip() for pred in decoded_preds]\n",
    "    cleaned_labels = [label.split(\"<|assistant|>\")[-1].strip() for label in decoded_labels]\n",
    "    \n",
    "    # Format for BLEU metric\n",
    "    references_for_bleu = [[label] for label in cleaned_labels]\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    result = bleu_metric.compute(predictions=cleaned_preds, references=references_for_bleu)\n",
    "    return {\"bleu\": result[\"bleu\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Run Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # Data collator for language modeling without masked language modeling (MLM)\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished. Best model is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T17:05:01.921715Z",
     "iopub.status.busy": "2025-06-06T17:05:01.921410Z",
     "iopub.status.idle": "2025-06-06T17:37:22.013940Z",
     "shell.execute_reply": "2025-06-06T17:37:22.013106Z",
     "shell.execute_reply.started": "2025-06-06T17:05:01.921694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define Stopping Criteria for Inference\n",
    "class StopOnEosToken(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0, -1] == tokenizer.eos_token_id: # Check if the last token is the end-of-sequence token\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Define Inference Function\n",
    "def generate_with_tinyllama(text):\n",
    "    prompt = (\n",
    "        f\"<|system|>\\nYou are an expert translator from ancient to modern Italian.{tokenizer.eos_token}\\n\" # System prompt to guide the model's behavior\n",
    "        f\"<|user|>\\n{text}{tokenizer.eos_token}\\n\" # User's input text to be translated\n",
    "        f\"<|assistant|>\\n\" # Placeholder for the model's response\n",
    "    )\n",
    "\n",
    "    # Tokenize the prompt and prepare inputs for the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnEosToken()])\n",
    "\n",
    "    # Generate the output using the model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=5, # Use beam search for better quality translations\n",
    "        early_stopping=True, # Stop generation when the model is confident enough\n",
    "        do_sample=False, # Disable sampling to ensure deterministic outputs\n",
    "        stopping_criteria=stopping_criteria, # Use custom stopping criteria to stop at the end-of-sequence token\n",
    "        repetition_penalty=1.2, # Penalizes words that have already appeared.\n",
    "        no_repeat_ngram_size=3, # Prevents the model from repeating any 3-word sequence.\n",
    "    )\n",
    "    \n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the assistant's response from the decoded output\n",
    "    try:\n",
    "        return decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "    except IndexError:\n",
    "        return f\"Model failed to generate a valid response. Full output: {decoded}\"\n",
    "\n",
    "# Run Inference on Test Set\n",
    "test_df = pd.read_csv('/inputs/dataset_human_eval.csv')[['Sentence', 'HumanEval']].dropna()\n",
    "test_df = test_df.rename(columns={'Sentence': 'ancient', 'HumanEval': 'modern'})\n",
    "test_df['tinyllama_output'] = [generate_with_tinyllama(text) for text in tqdm(test_df['ancient'], desc=\"Generating Final Translations\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T17:37:51.744739Z",
     "iopub.status.busy": "2025-06-06T17:37:51.743953Z",
     "iopub.status.idle": "2025-06-06T17:56:26.741584Z",
     "shell.execute_reply": "2025-06-06T17:56:26.740619Z",
     "shell.execute_reply.started": "2025-06-06T17:37:51.744714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating final test set: 100%|██████████| 97/97 [18:34<00:00, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions saved to /kaggle/working/tinyllama_final_dataset_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the Test Set\n",
    "final_output_test_file_path = '/inputs/dataset.csv'\n",
    "\n",
    "final_test_df = pd.read_csv(final_output_test_file_path)\n",
    "ancient_text_column_name = 'Sentence'\n",
    "\n",
    "final_test_df[ancient_text_column_name] = final_test_df[ancient_text_column_name].apply(fix_list_string_to_sentence)\n",
    "    \n",
    "# Filter out any empty sentences after cleaning, if any\n",
    "final_test_df = final_test_df[final_test_df[ancient_text_column_name].str.strip() != \"\"].copy()\n",
    "\n",
    "# Get the list of ancient sentences to translate\n",
    "ancient_sentences_to_translate = final_test_df[ancient_text_column_name].tolist()\n",
    "\n",
    "model.eval() # Ensure the model is in evaluation mode before generating translations\n",
    "generated_translations = [] # List to store generated translations\n",
    "print(\"Generating translations (this may take a while)...\") # Progress bar will show the progress of translation generation.\n",
    "for text in tqdm(ancient_sentences_to_translate, desc=\"Translating final test set\"):\n",
    "    generated_translations.append(generate_with_tinyllama(text))\n",
    "\n",
    "# Add the generated translations to the final test DataFrame\n",
    "final_test_df['generated_translation'] = generated_translations\n",
    "final_test_df['score_human'] = 0\n",
    "output_csv_path = \"/outputs/dataset_with_tinyllama_translations.csv\"\n",
    "output_columns = ['Author', 'Date', 'Region', 'Sentence', 'generated_translation', 'score_human']\n",
    "\n",
    "# Ensure the final DataFrame has the correct columns\n",
    "final_output_df_columns = [col for col in output_columns if col in final_test_df.columns]\n",
    "if 'generated_translation' not in final_output_df_columns:\n",
    "    final_output_df_columns.append('generated_translation')\n",
    "if 'score_human' not in final_output_df_columns:\n",
    "    final_output_df_columns.append('score_human')\n",
    "\n",
    "# Save the final DataFrame with translations to a CSV file\n",
    "final_test_df[final_output_df_columns].to_csv(output_csv_path, index=False)\n",
    "print(f\"Final predictions saved to {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7603016,
     "sourceId": 12077999,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
